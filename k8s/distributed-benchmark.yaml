apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-distributed-benchmark
  labels:
    app: mlperf
    type: distributed
    samples: "13368"
spec:
  parallelism: 2  # Run on 2 nodes simultaneously
  completions: 2
  backoffLimit: 1
  ttlSecondsAfterFinished: 86400
  template:
    metadata:
      labels:
        app: mlperf
        type: distributed
    spec:
      restartPolicy: Never
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: ["mlperf"]
              - key: type
                operator: In
                values: ["distributed"]
            topologyKey: kubernetes.io/hostname
      containers:
      - name: mlperf-distributed
        image: mlperf-universal:latest
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        args:
        - -c
        - |
          set -e
          echo "üöÄ Starting MLPerf Distributed Benchmark"
          echo "Node: ${NODE_NAME}"
          echo "Pod: ${POD_NAME}"
          echo "========================================="
          
          # Setup environment
          source /app/k8s-scripts/setup-cluster.sh
          
          # Run distributed benchmark with tensor parallelism
          export TENSOR_PARALLEL_SIZE=2
          
          # Wait for all pods to be ready
          echo "‚è≥ Waiting for distributed setup..."
          sleep 30
          
          # Run benchmark with distributed configuration
          python3 bin/run_distributed_benchmark.py \
            --samples ${SAMPLES} \
            --node ${NODE_NAME} \
            --distributed \
            --tensor-parallel-size 2
          
          echo "‚úÖ Distributed benchmark completed on ${NODE_NAME}"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: TENSOR_PARALLEL_SIZE
          value: "1"  # Will be overridden in args
        envFrom:
        - configMapRef:
            name: mlperf-config
        - secretRef:
            name: mlperf-secrets
            optional: true
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "20Gi"
            cpu: "6"
          limits:
            nvidia.com/gpu: 1
            memory: "40Gi"
            cpu: "12"
        volumeMounts:
        - name: shared-results
          mountPath: /shared
        - name: mlperf-scripts
          mountPath: /app/k8s-scripts
        - name: huggingface-cache
          mountPath: /root/.cache/huggingface
      volumes:
      - name: shared-results
        persistentVolumeClaim:
          claimName: mlperf-results-pvc
      - name: mlperf-scripts
        configMap:
          name: mlperf-scripts
          defaultMode: 0755
      - name: huggingface-cache
        persistentVolumeClaim:
          claimName: huggingface-cache-pvc