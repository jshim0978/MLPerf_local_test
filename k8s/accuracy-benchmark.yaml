apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-accuracy-benchmark
  labels:
    app: mlperf
    type: accuracy
    samples: "13368"
spec:
  backoffLimit: 1
  ttlSecondsAfterFinished: 86400
  template:
    metadata:
      labels:
        app: mlperf
        type: accuracy
    spec:
      restartPolicy: Never
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - name: mlperf-accuracy
        image: mlperf-universal:latest
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        args:
        - -c
        - |
          set -e
          echo "üéØ Starting MLPerf Accuracy Benchmark"
          echo "====================================="
          echo "Full dataset: ${SAMPLES} samples"
          echo "Model: ${MODEL_NAME}"
          echo "Precision: ${DTYPE}"
          echo ""
          
          # Setup environment
          source /app/k8s-scripts/setup-cluster.sh
          
          # Run accuracy benchmark
          python3 bin/run_benchmark.py \
            --samples ${SAMPLES} \
            --accuracy \
            --node local
          
          # Generate ROUGE metrics report
          echo "üìä Generating accuracy analysis..."
          python3 -c "
          import json
          import os
          from pathlib import Path
          
          # Find latest accuracy results
          results_dir = Path('${OUTPUT_DIR}')
          accuracy_files = list(results_dir.glob('**/mlperf_log_accuracy.json'))
          
          if accuracy_files:
              with open(accuracy_files[-1], 'r') as f:
                  accuracy_data = json.load(f)
              
              print(f'‚úÖ Accuracy benchmark completed')
              print(f'üìà Processed {len(accuracy_data)} samples')
              print(f'üíæ Results saved to {accuracy_files[-1]}')
          else:
              print('‚ö†Ô∏è No accuracy results found')
          "
          
          # Generate comprehensive accuracy report
          python3 report_generator.py --accuracy-only
          
          echo "‚úÖ Accuracy benchmark completed!"
          echo "üìä Detailed accuracy metrics available in reports/"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: ACCURACY
          value: "true"
        envFrom:
        - configMapRef:
            name: mlperf-config
        - secretRef:
            name: mlperf-secrets
            optional: true
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "24Gi"  # More memory for accuracy evaluation
            cpu: "6"
          limits:
            nvidia.com/gpu: 1
            memory: "48Gi"
            cpu: "12"
        volumeMounts:
        - name: shared-results
          mountPath: /shared
        - name: mlperf-scripts
          mountPath: /app/k8s-scripts
        - name: huggingface-cache
          mountPath: /root/.cache/huggingface
      volumes:
      - name: shared-results
        persistentVolumeClaim:
          claimName: mlperf-results-pvc
      - name: mlperf-scripts
        configMap:
          name: mlperf-scripts
          defaultMode: 0755
      - name: huggingface-cache
        persistentVolumeClaim:
          claimName: huggingface-cache-pvc