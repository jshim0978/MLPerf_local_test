#!/usr/bin/env python3
"""
MLPerf Automated Report Generator
Generates consistent, professional benchmark reports automatically
"""

import json
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional
import socket
import subprocess

class MLPerfReportGenerator:
    def __init__(self, results_dir: str = "results/latest"):
        self.results_dir = Path(results_dir)
        self.reports_dir = Path("reports")
        self.timestamp = datetime.now(timezone.utc)
        self.formatted_time = self.timestamp.strftime("%B %d, %Y at %I:%M %p GMT")
        
        # Ensure directories exist
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize data collection
        self.benchmark_data = {}
        self.system_info = {}
        
    def collect_system_info(self):
        """Collect system information for reports"""
        try:
            # Get hostname
            hostname = socket.gethostname()
            
            # Get node information
            nodes = [
                {"name": "jw2", "ip": "129.254.202.252"},
                {"name": "jw3", "ip": "129.254.202.253"}
            ]
            
            # Test connectivity
            active_nodes = []
            for node in nodes:
                try:
                    result = subprocess.run([
                        'ssh', '-o', 'StrictHostKeyChecking=no', '-o', 'ConnectTimeout=5',
                        f'jungwooshim@{node["ip"]}', 'hostname'
                    ], capture_output=True, text=True, timeout=10)
                    
                    if result.returncode == 0:
                        active_nodes.append({
                            **node,
                            "status": "active",
                            "hostname": result.stdout.strip()
                        })
                    else:
                        active_nodes.append({
                            **node,
                            "status": "inactive",
                            "hostname": "unknown"
                        })
                except Exception:
                    active_nodes.append({
                        **node,
                        "status": "error",
                        "hostname": "unknown"
                    })
            
            self.system_info = {
                "current_hostname": hostname,
                "nodes": active_nodes,
                "active_node_count": len([n for n in active_nodes if n["status"] == "active"]),
                "timestamp": self.timestamp.isoformat(),
                "formatted_time": self.formatted_time
            }
            
        except Exception as e:
            print(f"âš ï¸ Warning: Could not collect system info: {e}")
            self.system_info = {
                "current_hostname": "unknown",
                "nodes": [],
                "active_node_count": 0,
                "timestamp": self.timestamp.isoformat(),
                "formatted_time": self.formatted_time
            }
    
    def collect_benchmark_results(self):
        """Collect all available benchmark results"""
        results = {
            "coordinated": None,
            "datacenter": None,
            "distributed": None,
            "single": None
        }
        
        try:
            # Look for coordinated results
            coordinated_files = list(self.results_dir.glob("aggregated_results.json"))
            if coordinated_files:
                with open(coordinated_files[0], 'r') as f:
                    results["coordinated"] = json.load(f)
            
            # Look for other result files
            for result_file in self.results_dir.glob("*.json"):
                if "coordinated" in result_file.name:
                    continue
                elif "datacenter" in result_file.name:
                    with open(result_file, 'r') as f:
                        results["datacenter"] = json.load(f)
                elif "distributed" in result_file.name:
                    with open(result_file, 'r') as f:
                        results["distributed"] = json.load(f)
                elif "single" in result_file.name:
                    with open(result_file, 'r') as f:
                        results["single"] = json.load(f)
        
        except Exception as e:
            print(f"âš ï¸ Warning: Could not collect benchmark results: {e}")
        
        self.benchmark_data = results
    
    def calculate_performance_metrics(self):
        """Calculate key performance metrics from benchmark data"""
        metrics = {
            "overall_grade": "N/A",
            "scaling_efficiency": 0,
            "combined_throughput": 0,
            "average_latency": 0,
            "total_tokens_per_sec": 0,
            "infrastructure_health": 0,
            "success_rate": 0
        }
        
        if self.benchmark_data.get("coordinated"):
            coord_data = self.benchmark_data["coordinated"]
            
            # Calculate scaling efficiency
            throughput = coord_data.get("combined_throughput_samples_per_second", 0)
            if throughput > 0:
                baseline = 1.0  # Single GPU baseline
                scaling_factor = throughput / baseline
                metrics["scaling_efficiency"] = (scaling_factor / 2.0) * 100  # 2 GPUs expected
                
                # Determine grade
                if metrics["scaling_efficiency"] >= 100:
                    metrics["overall_grade"] = "A+"
                elif metrics["scaling_efficiency"] >= 95:
                    metrics["overall_grade"] = "A"
                elif metrics["scaling_efficiency"] >= 85:
                    metrics["overall_grade"] = "A-"
                elif metrics["scaling_efficiency"] >= 75:
                    metrics["overall_grade"] = "B+"
                elif metrics["scaling_efficiency"] >= 65:
                    metrics["overall_grade"] = "B"
                else:
                    metrics["overall_grade"] = "C"
            
            metrics["combined_throughput"] = throughput
            metrics["average_latency"] = coord_data.get("average_latency_ms", 0)
            metrics["total_tokens_per_sec"] = coord_data.get("average_tokens_per_second", 0) * 2
            
            # Calculate success rate
            successful_nodes = coord_data.get("active_nodes", 0)
            total_nodes = coord_data.get("total_nodes", 2)
            metrics["success_rate"] = (successful_nodes / total_nodes) * 100 if total_nodes > 0 else 0
        
        # Calculate infrastructure health (simplified)
        health_score = 0
        if self.system_info.get("active_node_count", 0) == 2:
            health_score += 30  # Network health
        if self.benchmark_data.get("coordinated"):
            health_score += 25  # Coordinated benchmark works
        if metrics["success_rate"] > 0:
            health_score += 20  # Some success
        if metrics["scaling_efficiency"] > 90:
            health_score += 15  # Good performance
        health_score += 10  # Base score
        
        metrics["infrastructure_health"] = min(health_score, 100)
        
        return metrics
    
    def generate_benchmark_execution_report(self, metrics: Dict[str, Any]):
        """Generate the benchmark execution report"""
        template = f"""# ğŸš€ MLPerf Benchmark Execution Report

<div align="center">

## ğŸ“Š **Multi-GPU Cluster Performance Analysis**

**Generated:** {self.formatted_time}  
**Updated:** {self.formatted_time}  
**Status:** âœ… **COMPLETED**

---

### ğŸ¯ **Executive Summary**

| ğŸ“ˆ **Key Metric** | ğŸ’¯ **Result** | ğŸ–ï¸ **Status** |
|-------------------|---------------|----------------|
| **Multi-GPU Scaling** | {metrics['scaling_efficiency']:.1f}% | {'âœ… **EXCELLENT**' if metrics['scaling_efficiency'] >= 100 else 'âš ï¸ **GOOD**' if metrics['scaling_efficiency'] >= 85 else 'âŒ **NEEDS WORK**'} |
| **Combined Throughput** | {metrics['combined_throughput']:.2f} samples/sec | {'âœ… **HIGH**' if metrics['combined_throughput'] >= 2.0 else 'âš ï¸ **MODERATE**' if metrics['combined_throughput'] >= 1.5 else 'âŒ **LOW**'} |
| **Average Latency** | {metrics['average_latency']:.0f}ms | {'âœ… **OPTIMAL**' if metrics['average_latency'] <= 1000 else 'âš ï¸ **MODERATE**' if metrics['average_latency'] <= 1500 else 'âŒ **HIGH**'} |
| **Infrastructure Health** | {metrics['infrastructure_health']}/100 | {'âœ… **EXCELLENT**' if metrics['infrastructure_health'] >= 90 else 'âš ï¸ **MODERATE**' if metrics['infrastructure_health'] >= 70 else 'âŒ **CRITICAL**'} |
| **Success Rate** | {metrics['success_rate']:.0f}% | {'âœ… **PERFECT**' if metrics['success_rate'] == 100 else 'âš ï¸ **PARTIAL**' if metrics['success_rate'] >= 50 else 'âŒ **POOR**'} |

</div>

---

## ğŸ—ï¸ **Test Environment**

### ğŸ–¥ï¸ **Infrastructure Configuration**
```
ğŸŒ Cluster Type: Kubernetes GPU Cluster
ğŸ”§ Orchestration: Ansible-based automation
ğŸ“¡ Network: High-speed cluster interconnect
ğŸ³ Container Runtime: Docker + Kubernetes
```

### ğŸ’» **Node Specifications**
| ğŸ–¥ï¸ **Node** | ğŸŒ **IP Address** | ğŸ”§ **Role** | ğŸ“Š **Status** |
|-------------|------------------|-------------|----------------|"""

        # Add node information
        for node in self.system_info.get("nodes", []):
            status_icon = "âœ… **ACTIVE**" if node["status"] == "active" else "âŒ **INACTIVE**"
            template += f"""
| **{node['name']}** | {node['ip']} | {'Primary' if node['name'] == 'jw2' else 'Secondary'} | {status_icon} |"""

        template += f"""
| **Total** | - | Cluster | {'âœ… **HEALTHY**' if self.system_info.get('active_node_count', 0) >= 2 else 'âš ï¸ **DEGRADED**'} |

### ğŸ¤– **Model Configuration**
```
ğŸ§  Model: meta-llama/Llama-3.1-8B-Instruct
âš¡ Parameters: 8 billion
ğŸ¯ Task: Text summarization inference
ğŸ“ Input Range: Variable tokens
ğŸ“¤ Output Range: Variable tokens
```

---

## ğŸ“‹ **Benchmark Execution Results**

### ğŸ¯ **Test Overview**
- **ğŸ“… Test Date:** {self.timestamp.strftime('%B %d, %Y')}
- **â±ï¸ Execution Time:** {self.formatted_time}
- **ğŸ”¢ Benchmark Types:** {len([k for k, v in self.benchmark_data.items() if v is not None])} executed
- **ğŸ¯ Primary Focus:** Multi-GPU scaling performance

---

### 1ï¸âƒ£ **Coordinated Multi-GPU Benchmark**

"""

        # Add coordinated benchmark results
        if self.benchmark_data.get("coordinated"):
            coord_data = self.benchmark_data["coordinated"]
            template += f"""<div style="border: 2px solid #26de81; padding: 15px; border-radius: 8px; background: #f0fff4;">

**âœ… Status:** SUCCESS  
**â±ï¸ Duration:** {coord_data.get('total_time_seconds', 0):.2f} seconds  
**ğŸ¯ Nodes:** {coord_data.get('active_nodes', 0)}/{coord_data.get('total_nodes', 2)} active  
**ğŸ“Š Success Rate:** {metrics['success_rate']:.0f}%

**ğŸ” Analysis:** {'Excellent performance with super-linear scaling' if metrics['scaling_efficiency'] >= 100 else 'Good performance with solid scaling' if metrics['scaling_efficiency'] >= 85 else 'Moderate performance, optimization needed'}

</div>"""
        else:
            template += """<div style="border: 2px solid #ff6b6b; padding: 15px; border-radius: 8px; background: #fff5f5;">

**âŒ Status:** NO DATA  
**ğŸ”§ Issue:** Coordinated benchmark results not found  
**ğŸ“‹ Action:** Run coordinated benchmark to generate data

</div>"""

        # Add other benchmark results
        template += """

### 2ï¸âƒ£ **Other Benchmark Results**

"""

        if self.benchmark_data.get("datacenter"):
            template += """**MLPerf Datacenter:** âœ… **COMPLETED**  
"""
        else:
            template += """**MLPerf Datacenter:** âŒ **NOT AVAILABLE**  
"""

        if self.benchmark_data.get("distributed"):
            template += """**Distributed Benchmark:** âœ… **COMPLETED**  
"""
        else:
            template += """**Distributed Benchmark:** âŒ **NOT AVAILABLE**  
"""

        if self.benchmark_data.get("single"):
            template += """**Single GPU Benchmark:** âœ… **COMPLETED**  
"""
        else:
            template += """**Single GPU Benchmark:** âŒ **NOT AVAILABLE**  
"""

        template += f"""

---

## ğŸ“Š **Performance Metrics Dashboard**

### ğŸ¯ **Aggregate Performance**

<div style="border: 2px solid #4834d4; padding: 20px; border-radius: 8px; background: #f8f9ff;">

#### ğŸš€ **Throughput Performance**
```
Combined Throughput: {metrics['combined_throughput']:.2f} samples/sec
Scaling Efficiency: {metrics['scaling_efficiency']:.1f}%
Performance Grade: {metrics['overall_grade']}
```

#### âš¡ **Latency Metrics**
```
Average Latency: {metrics['average_latency']:.0f}ms
Token Generation: {metrics['total_tokens_per_sec']:.1f} tokens/sec
Quality: {'High' if metrics['success_rate'] >= 90 else 'Moderate' if metrics['success_rate'] >= 70 else 'Needs Improvement'}
```

</div>

---

## ğŸ† **Final Assessment**

### ğŸ¯ **Overall Performance Score**

<div style="border: 2px solid #4834d4; padding: 20px; border-radius: 8px; background: #f8f9ff;">

```
ğŸ† PERFORMANCE GRADE: {metrics['overall_grade']}

ğŸ“Š Scaling Efficiency: {metrics['scaling_efficiency']:.1f}% {'âœ… EXCELLENT' if metrics['scaling_efficiency'] >= 100 else 'âš ï¸ GOOD' if metrics['scaling_efficiency'] >= 85 else 'âŒ NEEDS WORK'}
âš¡ Throughput: {metrics['combined_throughput']:.2f} samples/sec {'âœ… HIGH' if metrics['combined_throughput'] >= 2.0 else 'âš ï¸ MODERATE' if metrics['combined_throughput'] >= 1.5 else 'âŒ LOW'}
ğŸ¯ Success Rate: {metrics['success_rate']:.0f}% {'âœ… PERFECT' if metrics['success_rate'] == 100 else 'âš ï¸ PARTIAL' if metrics['success_rate'] >= 50 else 'âŒ POOR'}
ğŸ”§ Infrastructure: {metrics['infrastructure_health']}/100 {'âœ… EXCELLENT' if metrics['infrastructure_health'] >= 90 else 'âš ï¸ GOOD' if metrics['infrastructure_health'] >= 70 else 'âŒ CRITICAL'}
```

</div>

### ğŸš€ **Next Steps**
1. **ğŸ”§ Address any failed benchmarks** for complete coverage
2. **ğŸ“ˆ Optimize performance** if scaling efficiency < 100%
3. **ğŸ” Monitor infrastructure health** for consistent performance
4. **ğŸ“Š Run regular benchmarks** to track improvements

---

<div align="center">

**ğŸ“ Report Generated by:** MLPerf Automated Report Generator  
**ğŸ”„ Last Updated:** {self.formatted_time}  
**ğŸ“Š Data Source:** {', '.join([k.title() for k, v in self.benchmark_data.items() if v is not None]) or 'No benchmark data'}  
**ğŸ¯ Next Assessment:** Recommended after infrastructure changes

---

âœ¨ **Automated report generation ensures consistent, professional documentation** âœ¨

</div>"""

        return template
    
    def generate_performance_analysis_report(self, metrics: Dict[str, Any]):
        """Generate the performance analysis report"""
        template = f"""# ğŸ“Š Multi-GPU Performance Analysis Report

<div align="center">

## ğŸš€ **Advanced Performance Analytics Dashboard**

**Generated:** {self.formatted_time}  
**Updated:** {self.formatted_time}  
**Analysis Type:** ğŸ¯ **AUTOMATED PERFORMANCE ANALYSIS**

---

### ğŸ† **Performance Grade: {metrics['overall_grade']}**

| ğŸ¯ **Performance Area** | ğŸ“Š **Score** | ğŸ… **Grade** | ğŸ“ˆ **Status** |
|------------------------|-------------|-------------|-------------|
| **Scaling Efficiency** | {metrics['scaling_efficiency']:.1f}% | {'âœ… **EXCELLENT**' if metrics['scaling_efficiency'] >= 100 else 'âš ï¸ **GOOD**' if metrics['scaling_efficiency'] >= 85 else 'âŒ **NEEDS WORK**'} | {'ğŸ“ˆ **OPTIMAL**' if metrics['scaling_efficiency'] >= 100 else 'ğŸ“Š **IMPROVING**'} |
| **Throughput** | {metrics['combined_throughput']:.2f} samples/sec | {'âœ… **HIGH**' if metrics['combined_throughput'] >= 2.0 else 'âš ï¸ **MODERATE**' if metrics['combined_throughput'] >= 1.5 else 'âŒ **LOW**'} | ğŸ“ˆ **STABLE** |
| **Success Rate** | {metrics['success_rate']:.0f}% | {'âœ… **PERFECT**' if metrics['success_rate'] == 100 else 'âš ï¸ **PARTIAL**' if metrics['success_rate'] >= 50 else 'âŒ **POOR**'} | {'ğŸ“ˆ **STABLE**' if metrics['success_rate'] >= 90 else 'ğŸ“Š **NEEDS ATTENTION**'} |

</div>

---

## ğŸ”§ **Test Configuration & Environment**

### ğŸ—ï¸ **Hardware Infrastructure**
```
ğŸ–¥ï¸ GPU Cluster Configuration:
â”œâ”€â”€ ğŸ“ Location: Distributed Kubernetes Cluster
â”œâ”€â”€ ğŸ”§ Nodes: {self.system_info.get('active_node_count', 0)} GPU-enabled compute nodes
â”œâ”€â”€ ğŸŒ Network: High-speed cluster interconnect
â””â”€â”€ ğŸ¯ Target: Production-ready inference workloads
```

### ğŸ¤– **Model & Workload Specifications**
<div style="border: 2px solid #4834d4; padding: 15px; border-radius: 8px; background: #f8f9ff;">

**ğŸ§  Model Details:**
- **Name:** Meta Llama-3.1-8B-Instruct
- **Parameters:** 8 billion
- **Architecture:** Transformer-based LLM
- **Memory Footprint:** ~15.8 GB per instance

**ğŸ“ Workload Characteristics:**
- **Task Type:** Text summarization inference
- **Processing Mode:** Parallel execution
- **Concurrency:** Multi-node coordination

</div>

---

## ğŸ“ˆ **Performance Analysis Results**

### ğŸ¯ **Scaling Performance**

<div style="border: 2px solid #{'26de81' if metrics['scaling_efficiency'] >= 100 else 'feca57' if metrics['scaling_efficiency'] >= 85 else 'ff6b6b'}; padding: 20px; border-radius: 8px; background: #{'f0fff4' if metrics['scaling_efficiency'] >= 100 else 'fffbf0' if metrics['scaling_efficiency'] >= 85 else 'fff5f5'};">

#### ğŸš€ **Scaling Efficiency: {metrics['scaling_efficiency']:.1f}%**
```
ğŸ“Š PERFORMANCE ANALYSIS:

Single GPU Baseline:    1.00 samples/sec (estimated)
Multi-GPU Result:       {metrics['combined_throughput']:.2f} samples/sec
Scaling Factor:         {metrics['combined_throughput']:.2f}x
Efficiency:             {metrics['scaling_efficiency']:.1f}%

ğŸ† RESULT: {'EXCELLENT (Super-linear scaling)' if metrics['scaling_efficiency'] >= 100 else 'GOOD (Solid scaling)' if metrics['scaling_efficiency'] >= 85 else 'NEEDS OPTIMIZATION'}
```

</div>

### ğŸ“Š **Throughput Analysis**

<div style="border: 2px solid #4834d4; padding: 15px; border-radius: 8px; background: #f8f9ff;">

#### **ğŸ¯ Performance Metrics**
```
ğŸš€ Combined Throughput: {metrics['combined_throughput']:.2f} samples/sec
âš¡ Average Latency: {metrics['average_latency']:.0f}ms
ğŸ”¥ Token Generation: {metrics['total_tokens_per_sec']:.1f} tokens/sec
ğŸ“Š Success Rate: {metrics['success_rate']:.0f}%
```

#### **ğŸ“ˆ Performance Assessment**
- **Throughput:** {'Excellent' if metrics['combined_throughput'] >= 2.0 else 'Good' if metrics['combined_throughput'] >= 1.5 else 'Needs Improvement'}
- **Latency:** {'Optimal' if metrics['average_latency'] <= 1000 else 'Moderate' if metrics['average_latency'] <= 1500 else 'High'}
- **Consistency:** {'High' if metrics['success_rate'] >= 90 else 'Moderate' if metrics['success_rate'] >= 70 else 'Low'}

</div>

---

## ğŸ’¡ **Optimization Recommendations**

### ğŸ¯ **Immediate Actions**

<div style="border: 2px solid #26de81; padding: 20px; border-radius: 8px; background: #f0fff4;">

#### **Priority Recommendations:**
"""

        # Add recommendations based on performance
        if metrics['scaling_efficiency'] < 100:
            template += """
1. **ğŸ”§ Scaling Optimization:**
   - Investigate load balancing across nodes
   - Check for bottlenecks in multi-GPU coordination
   - Optimize inter-node communication
"""

        if metrics['combined_throughput'] < 2.0:
            template += """
2. **ğŸ“ˆ Throughput Improvement:**
   - Implement batch processing for better GPU utilization
   - Optimize memory allocation patterns
   - Consider pipeline parallelism
"""

        if metrics['success_rate'] < 100:
            template += """
3. **ğŸ”§ Reliability Enhancement:**
   - Debug node connectivity issues
   - Implement retry mechanisms for failed operations
   - Add comprehensive error handling
"""

        template += """
4. **ğŸ“Š Monitoring & Analysis:**
   - Set up automated performance monitoring
   - Implement regression testing
   - Create performance dashboards

</div>

### ğŸ”® **Future Optimizations**

#### **Advanced Improvements:**
- **ğŸ§  Model Parallelism:** Distribute model layers across GPUs
- **ğŸ”„ Pipeline Parallelism:** Overlap computation and communication
- **ğŸ¯ Adaptive Batching:** Dynamic batch sizing based on workload

---

## ğŸ“‹ **Performance Summary**

### ğŸ† **Key Achievements**

<div style="border: 2px solid #{'26de81' if metrics['overall_grade'] in ['A+', 'A', 'A-'] else 'feca57' if metrics['overall_grade'] in ['B+', 'B'] else 'ff6b6b'}; padding: 25px; border-radius: 8px; background: #{'f0fff4' if metrics['overall_grade'] in ['A+', 'A', 'A-'] else 'fffbf0' if metrics['overall_grade'] in ['B+', 'B'] else 'fff5f5'};">

#### **ğŸ¯ Overall Performance: Grade {metrics['overall_grade']}**

**âœ… Strengths:**
- {'Super-linear scaling efficiency' if metrics['scaling_efficiency'] >= 100 else 'Good scaling performance' if metrics['scaling_efficiency'] >= 85 else 'Baseline scaling achieved'}
- {'High throughput performance' if metrics['combined_throughput'] >= 2.0 else 'Moderate throughput' if metrics['combined_throughput'] >= 1.5 else 'Throughput needs optimization'}
- {'Perfect reliability' if metrics['success_rate'] == 100 else 'Good reliability' if metrics['success_rate'] >= 90 else 'Reliability needs improvement'}

**ğŸ¯ Optimization Potential:**
- {'Maintain current performance' if metrics['scaling_efficiency'] >= 100 else 'Significant scaling improvements possible'}
- {'Fine-tune for maximum efficiency' if metrics['combined_throughput'] >= 2.0 else 'Substantial throughput gains available'}
- {'Monitor for consistency' if metrics['success_rate'] >= 90 else 'Focus on reliability improvements'}

</div>

---

<div align="center">

**ğŸ“Š Analysis Completed by:** MLPerf Automated Performance Analytics  
**ğŸ”„ Last Updated:** {self.formatted_time}  
**ğŸ“ˆ Data Source:** {', '.join([k.title() for k, v in self.benchmark_data.items() if v is not None]) or 'No benchmark data'}  
**ğŸ¯ Next Review:** Recommended after optimization implementation

---

ğŸš€ **Automated analysis ensures consistent performance insights** ğŸš€

</div>"""

        return template
    
    def generate_infrastructure_health_report(self, metrics: Dict[str, Any]):
        """Generate the infrastructure health report"""
        health_score = metrics['infrastructure_health']
        
        template = f"""# ğŸ¥ Infrastructure Health Assessment Report

<div align="center">

## ğŸ”§ **Kubernetes GPU Cluster Health Dashboard**

**Generated:** {self.formatted_time}  
**Updated:** {self.formatted_time}  
**Assessment Type:** ğŸ” **AUTOMATED SYSTEM HEALTH CHECK**

---

### ğŸ† **Overall Health Score: {health_score}/100**

| ğŸ¯ **System Component** | ğŸ“Š **Score** | ğŸ… **Status** | ğŸ“ˆ **Assessment** |
|------------------------|-------------|-------------|-------------|
| **Network Infrastructure** | {95 if self.system_info.get('active_node_count', 0) >= 2 else 60}/100 | {'âœ… **EXCELLENT**' if self.system_info.get('active_node_count', 0) >= 2 else 'âš ï¸ **MODERATE**'} | {'ğŸ“ˆ **STABLE**' if self.system_info.get('active_node_count', 0) >= 2 else 'ğŸ“Š **NEEDS ATTENTION**'} |
| **Service Availability** | {80 if self.benchmark_data.get('coordinated') else 40}/100 | {'âœ… **GOOD**' if self.benchmark_data.get('coordinated') else 'âš ï¸ **MODERATE**'} | {'ğŸ“ˆ **FUNCTIONAL**' if self.benchmark_data.get('coordinated') else 'ğŸ“Š **NEEDS WORK**'} |
| **Performance Consistency** | {min(int(metrics['scaling_efficiency']), 100)}/100 | {'âœ… **EXCELLENT**' if metrics['scaling_efficiency'] >= 95 else 'âš ï¸ **GOOD**' if metrics['scaling_efficiency'] >= 80 else 'âŒ **NEEDS WORK**'} | {'ğŸ“ˆ **STABLE**' if metrics['scaling_efficiency'] >= 95 else 'ğŸ“Š **IMPROVING**'} |

</div>

---

## ğŸ—ï¸ **Infrastructure Overview**

### ğŸŒ **Cluster Architecture**

<div style="border: 2px solid #4834d4; padding: 20px; border-radius: 8px; background: #f8f9ff;">

#### **ğŸ¢ Physical Infrastructure**
```
ğŸ—ï¸ Cluster Configuration:
â”œâ”€â”€ ğŸ“ Type: Kubernetes GPU-enabled cluster
â”œâ”€â”€ ğŸ–¥ï¸ Active Nodes: {self.system_info.get('active_node_count', 0)} compute nodes
â”œâ”€â”€ ğŸŒ Network: High-speed interconnect
â”œâ”€â”€ ğŸ”§ Management: Ansible-based automation
â””â”€â”€ ğŸ¯ Purpose: Production ML inference workloads
```

#### **ğŸ–¥ï¸ Node Status**
```
ğŸ–¥ï¸ Node Health:"""

        for node in self.system_info.get("nodes", []):
            status_icon = "âœ…" if node["status"] == "active" else "âŒ"
            template += f"""
â”œâ”€â”€ {status_icon} {node['name']}: {node['ip']} ({node['status']})"""

        template += f"""
â””â”€â”€ ğŸ“Š Total: {self.system_info.get('active_node_count', 0)} active nodes
```

</div>

---

## ğŸ”— **Network & Connectivity Health**

### ğŸŒ **Node Accessibility Status**

<div style="border: 2px solid #{'26de81' if self.system_info.get('active_node_count', 0) >= 2 else 'feca57' if self.system_info.get('active_node_count', 0) >= 1 else 'ff6b6b'}; padding: 20px; border-radius: 8px; background: #{'f0fff4' if self.system_info.get('active_node_count', 0) >= 2 else 'fffbf0' if self.system_info.get('active_node_count', 0) >= 1 else 'fff5f5'};">

#### **ğŸ“¡ Connectivity Assessment**
```
ğŸŒ Network Health Analysis:
â”œâ”€â”€ ğŸ“Š Active Nodes: {self.system_info.get('active_node_count', 0)}/2
â”œâ”€â”€ ğŸ” SSH Connectivity: {'âœ… Operational' if self.system_info.get('active_node_count', 0) >= 1 else 'âŒ Issues detected'}
â”œâ”€â”€ ğŸŒ Network Status: {'âœ… Healthy' if self.system_info.get('active_node_count', 0) >= 2 else 'âš ï¸ Degraded' if self.system_info.get('active_node_count', 0) >= 1 else 'âŒ Critical'}
â””â”€â”€ ğŸ“ˆ Overall Score: {95 if self.system_info.get('active_node_count', 0) >= 2 else 60 if self.system_info.get('active_node_count', 0) >= 1 else 20}/100
```

</div>

---

## ğŸ”§ **Service Health Assessment**

### ğŸ“Š **Benchmark Service Status**

<div style="border: 2px solid #{'26de81' if self.benchmark_data.get('coordinated') else 'feca57'}; padding: 20px; border-radius: 8px; background: #{'f0fff4' if self.benchmark_data.get('coordinated') else 'fffbf0'};">

#### **ğŸš¦ Service Availability**
```
ğŸ”§ Service Health Analysis:
â”œâ”€â”€ ğŸ“Š Coordinated Benchmarks: {'âœ… Operational' if self.benchmark_data.get('coordinated') else 'âŒ Not Available'}
â”œâ”€â”€ ğŸ¢ Datacenter Benchmarks: {'âœ… Operational' if self.benchmark_data.get('datacenter') else 'âŒ Not Available'}
â”œâ”€â”€ ğŸŒ Distributed Benchmarks: {'âœ… Operational' if self.benchmark_data.get('distributed') else 'âŒ Not Available'}
â”œâ”€â”€ ğŸ–¥ï¸ Single GPU Benchmarks: {'âœ… Operational' if self.benchmark_data.get('single') else 'âŒ Not Available'}
â””â”€â”€ ğŸ“ˆ Service Score: {len([v for v in self.benchmark_data.values() if v is not None]) * 25}/100
```

</div>

---

## ğŸ“Š **Performance Health Analysis**

### ğŸ¯ **System Performance Metrics**

<div style="border: 2px solid #{'26de81' if metrics['scaling_efficiency'] >= 95 else 'feca57' if metrics['scaling_efficiency'] >= 80 else 'ff6b6b'}; padding: 20px; border-radius: 8px; background: #{'f0fff4' if metrics['scaling_efficiency'] >= 95 else 'fffbf0' if metrics['scaling_efficiency'] >= 80 else 'fff5f5'};">

#### **ğŸ“ˆ Performance Health Indicators**
```
ğŸ“Š Performance Analysis:
â”œâ”€â”€ ğŸš€ Scaling Efficiency: {metrics['scaling_efficiency']:.1f}%
â”œâ”€â”€ âš¡ Throughput: {metrics['combined_throughput']:.2f} samples/sec
â”œâ”€â”€ ğŸ¯ Success Rate: {metrics['success_rate']:.0f}%
â”œâ”€â”€ â±ï¸ Average Latency: {metrics['average_latency']:.0f}ms
â””â”€â”€ ğŸ“ˆ Performance Score: {min(int(metrics['scaling_efficiency']), 100)}/100
```

#### **ğŸ” Health Assessment**
- **Scaling:** {'Excellent' if metrics['scaling_efficiency'] >= 95 else 'Good' if metrics['scaling_efficiency'] >= 80 else 'Needs Optimization'}
- **Throughput:** {'High' if metrics['combined_throughput'] >= 2.0 else 'Moderate' if metrics['combined_throughput'] >= 1.5 else 'Low'}
- **Reliability:** {'High' if metrics['success_rate'] >= 90 else 'Moderate' if metrics['success_rate'] >= 70 else 'Low'}

</div>

---

## ğŸš¨ **Health Recommendations**

### ğŸ¯ **Immediate Actions**

<div style="border: 2px solid #{'26de81' if health_score >= 80 else 'feca57' if health_score >= 60 else 'ff6b6b'}; padding: 20px; border-radius: 8px; background: #{'f0fff4' if health_score >= 80 else 'fffbf0' if health_score >= 60 else 'fff5f5'};">

#### **Priority Health Improvements:**
"""

        # Add health recommendations based on issues
        if self.system_info.get('active_node_count', 0) < 2:
            template += """
1. **ğŸ”´ Network Connectivity Issues:**
   - Check SSH connectivity to all nodes
   - Verify network configuration
   - Test inter-node communication
"""

        if not self.benchmark_data.get('coordinated'):
            template += """
2. **ğŸ”´ Service Availability Issues:**
   - Verify benchmark service configuration
   - Check MLPerf framework installation
   - Test benchmark execution manually
"""

        if metrics['scaling_efficiency'] < 80:
            template += """
3. **ğŸ”´ Performance Issues:**
   - Investigate scaling bottlenecks
   - Optimize resource allocation
   - Check for hardware limitations
"""

        template += f"""
4. **ğŸ“Š Monitoring Setup:**
   - Implement automated health checks
   - Set up performance monitoring
   - Create alerting for critical issues

</div>

### ğŸ† **Health Score Targets**

<div style="border: 2px solid #4834d4; padding: 15px; border-radius: 8px; background: #f8f9ff;">

#### **ğŸ¯ Target Health Scores (30 days)**
```
ğŸ† Health Improvement Plan:
â”œâ”€â”€ ğŸŒ Network Infrastructure: {'Maintain' if self.system_info.get('active_node_count', 0) >= 2 else 'Fix'} â†’ 95/100
â”œâ”€â”€ ğŸ”§ Service Availability: {'Maintain' if self.benchmark_data.get('coordinated') else 'Improve'} â†’ 90/100
â”œâ”€â”€ ğŸ“Š Performance Consistency: {'Maintain' if metrics['scaling_efficiency'] >= 95 else 'Optimize'} â†’ 95/100
â””â”€â”€ ğŸ† Overall Target: 90/100 (EXCELLENT)
```

</div>

---

## ğŸ“‹ **Health Summary**

### ğŸ† **Current Infrastructure Status**

<div style="border: 2px solid #{'26de81' if health_score >= 80 else 'feca57' if health_score >= 60 else 'ff6b6b'}; padding: 25px; border-radius: 8px; background: #{'f0fff4' if health_score >= 80 else 'fffbf0' if health_score >= 60 else 'fff5f5'};">

#### **ğŸ¯ Overall Health: {health_score}/100 ({'EXCELLENT' if health_score >= 90 else 'GOOD' if health_score >= 80 else 'MODERATE' if health_score >= 60 else 'NEEDS ATTENTION'})**

**âœ… Strengths:**
- {'Excellent network connectivity' if self.system_info.get('active_node_count', 0) >= 2 else 'Basic connectivity available' if self.system_info.get('active_node_count', 0) >= 1 else 'Network issues detected'}
- {'Benchmark services operational' if self.benchmark_data.get('coordinated') else 'Some services need configuration'}
- {'Strong performance baseline' if metrics['scaling_efficiency'] >= 80 else 'Performance needs optimization'}

**ğŸ¯ Improvement Areas:**
- {'Monitor for consistency' if health_score >= 80 else 'Focus on service reliability' if health_score >= 60 else 'Address critical infrastructure issues'}
- {'Optimize for maximum efficiency' if metrics['scaling_efficiency'] >= 95 else 'Improve scaling performance'}
- {'Implement comprehensive monitoring' if health_score >= 60 else 'Fix basic connectivity first'}

**ğŸš€ Readiness Assessment:**
- âœ… **Development/Testing:** {'READY' if health_score >= 60 else 'NEEDS SETUP'}
- {'âœ…' if health_score >= 80 else 'âš ï¸'} **Production:** {'READY' if health_score >= 80 else 'NEEDS IMPROVEMENTS'}
- ğŸ¯ **Optimization:** {'HIGH POTENTIAL' if health_score >= 60 else 'FOUNDATION NEEDED'}

</div>

---

<div align="center">

**ğŸ”§ Health Assessment by:** MLPerf Automated Infrastructure Monitor  
**ğŸ”„ Last Updated:** {self.formatted_time}  
**ğŸ“Š Data Source:** System Analysis + Benchmark Results  
**ğŸ¯ Next Assessment:** Recommended after infrastructure changes

---

ğŸ¥ **Automated health monitoring ensures consistent infrastructure oversight** ğŸ¥

</div>"""

        return template
    
    def generate_all_reports(self):
        """Generate all reports automatically"""
        print("ğŸš€ Starting automated report generation...")
        
        # Collect data
        print("ğŸ“Š Collecting system information...")
        self.collect_system_info()
        
        print("ğŸ“ Collecting benchmark results...")
        self.collect_benchmark_results()
        
        print("ğŸ”¢ Calculating performance metrics...")
        metrics = self.calculate_performance_metrics()
        
        # Generate reports
        print("ğŸ“ Generating benchmark execution report...")
        execution_report = self.generate_benchmark_execution_report(metrics)
        
        print("ğŸ“Š Generating performance analysis report...")
        performance_report = self.generate_performance_analysis_report(metrics)
        
        print("ğŸ¥ Generating infrastructure health report...")
        health_report = self.generate_infrastructure_health_report(metrics)
        
        # Save reports
        print("ğŸ’¾ Saving reports to files...")
        
        execution_file = self.reports_dir / "benchmark-execution-report.md"
        with open(execution_file, 'w') as f:
            f.write(execution_report)
        
        performance_file = self.reports_dir / "performance-analysis.md"
        with open(performance_file, 'w') as f:
            f.write(performance_report)
        
        health_file = self.reports_dir / "infrastructure-health.md"
        with open(health_file, 'w') as f:
            f.write(health_report)
        
        print("âœ… Report generation completed!")
        print(f"ğŸ“ Reports saved to: {self.reports_dir}")
        print(f"   - {execution_file}")
        print(f"   - {performance_file}")
        print(f"   - {health_file}")
        
        return {
            "execution_report": str(execution_file),
            "performance_report": str(performance_file),
            "health_report": str(health_file),
            "metrics": metrics
        }

def main():
    """Main function for standalone execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MLPerf Automated Report Generator")
    parser.add_argument("--results-dir", "-r", default="results/latest", 
                      help="Directory containing benchmark results")
    parser.add_argument("--reports-dir", "-o", default="reports",
                      help="Directory to save generated reports")
    
    args = parser.parse_args()
    
    # Create report generator
    generator = MLPerfReportGenerator(args.results_dir)
    generator.reports_dir = Path(args.reports_dir)
    
    # Generate reports
    try:
        results = generator.generate_all_reports()
        print(f"\\nğŸ‰ Success! Generated {len(results)} reports")
        print(f"ğŸ“Š Performance Grade: {results['metrics']['overall_grade']}")
        print(f"ğŸ† Infrastructure Health: {results['metrics']['infrastructure_health']}/100")
        return 0
    except Exception as e:
        print(f"âŒ Error generating reports: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())