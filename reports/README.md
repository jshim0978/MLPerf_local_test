# MLPerf Benchmark Reports

This directory contains all generated benchmark reports and visualizations.

## ğŸ“ Directory Structure

```
reports/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ examples/                           # Sample outputs for reference
â”‚   â”œâ”€â”€ sample_benchmark_report.md      # Complete benchmark analysis report
â”‚   â”œâ”€â”€ sample_visualization.png        # Performance visualization charts
â”‚   â”œâ”€â”€ sample_accuracy_test.json       # Accuracy test results (JSON)
â”‚   â”œâ”€â”€ sample_performance_test.json    # Performance test results (JSON)
â”‚   â””â”€â”€ sample_performance_analysis.png # Legacy performance charts
â””â”€â”€ [your_benchmark_results]            # Your actual benchmark outputs will appear here
```

## ğŸ¯ Expected Outputs

When you run benchmarks using `python3 bin/run_benchmark.py`, the following files will be generated:

### 1. **Benchmark Reports** (`.md` files)
- **Format**: `local_benchmark_YYYYMMDD_HHMMSS.md`
- **Content**: Configuration details, performance metrics, file listings
- **Example**: See `examples/sample_benchmark_report.md`

### 2. **JSON Results** (`.json` files)
- **Format**: `local_benchmark_YYYYMMDD_HHMMSS.json`
- **Content**: Structured data with timestamps, durations, paths
- **Example**: See `examples/sample_accuracy_test.json`

### 3. **Visualizations** (`.png` files)
Generated when using the visualization tools:
- **Performance charts**: Multi-panel analysis with throughput, latency breakdown
- **Comparison graphs**: Test comparisons and key metrics summary
- **Example**: See `examples/sample_visualization.png`

## ğŸ“Š Sample Performance Metrics

Based on the example results (NVIDIA A30, Llama-3.1-8B):

| Metric | Value | Unit |
|--------|-------|------|
| **Throughput** | 0.34 | samples/sec |
| **Token Generation** | 44.52 | tokens/sec |
| **Mean Latency** | 53.18 | seconds |
| **First Token Latency** | 50.39 | seconds |
| **Per Token Time** | 21.96 | ms |

## ğŸš€ Quick Start

1. **Run a benchmark**:
   ```bash
   python3 bin/run_benchmark.py --samples 10
   ```

2. **Generate visualizations**:
   ```bash
   python3 benchmark_results_report.py
   ```

3. **Check results**:
   ```bash
   ls reports/
   ```

## ğŸ“ Report Types

### Accuracy Reports
- **Purpose**: Validate model output quality
- **Content**: Sample processing, token counts, accuracy metrics
- **Usage**: `--accuracy` flag during benchmark

### Performance Reports  
- **Purpose**: Measure throughput and latency
- **Content**: Samples/sec, tokens/sec, latency percentiles
- **Usage**: Default benchmark mode

### Comprehensive Reports
- **Purpose**: Complete analysis with visualizations
- **Content**: Combined metrics, charts, deployment recommendations
- **Usage**: Generated by `benchmark_results_report.py`

## ğŸ”§ Customization

To modify report generation:
- Edit `bin/run_benchmark.py` for basic reports
- Edit `benchmark_results_report.py` for visualizations
- Check `src/config.py` for output directory settings

---

**Note**: All timestamps in filenames use UTC format `YYYYMMDD_HHMMSS` for consistent sorting and organization.