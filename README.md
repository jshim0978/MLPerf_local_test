# MLPerf Inference Benchmark Suite

**Official MLCommons Llama-3.1-8B inference benchmarking with automatic visual reporting**

## Overview

This repository contains the **official MLCommons MLPerf inference implementation** for Llama-3.1-8B benchmarking with enhanced visual reporting capabilities. All benchmarks use the genuine MLPerf loadgen with the complete CNN DailyMail dataset (13,368 samples).

## Key Features

- âœ… **Official Implementation**: Uses genuine MLCommons reference code
- âœ… **Complete Dataset**: Full CNN DailyMail dataset (13,368 samples, not synthetic)
- âœ… **Auto Visual Reports**: Comprehensive charts generated automatically when benchmarks complete
- âœ… **Multi-GPU Support**: Distributed benchmarking across NVIDIA A30 GPUs
- âœ… **Production Ready**: VLLM optimization with proper MLPerf compliance

## Quick Start

### 1. Configure Your Infrastructure
```bash
# Create configuration for your specific infrastructure
python3 config_manager.py --create-example
cp config.yaml.example config.yaml

# Edit config.yaml with your GPU node IPs, usernames, etc.
nano config.yaml

# Validate configuration
python3 config_manager.py --validate
```

### 2. Generate Custom Scripts
```bash
# Generate scripts customized for your infrastructure
python3 config_manager.py --generate-scripts

# This creates infrastructure-specific scripts:
# - run_benchmarks.sh (for your specific nodes)
# - monitor_benchmarks.sh (monitors your nodes)
```

### 3. Run Benchmarks
```bash
# Start benchmarks on your configured infrastructure
./run_benchmarks.sh

# Monitor progress with auto-reporting
./monitor_benchmarks.sh watch

# Generate visual reports anytime
python3 generate_visual_reports.py results
```

## Repository Structure

```
â”œâ”€â”€ official_mlperf/              # Official MLCommons implementation
â”‚   â”œâ”€â”€ main.py                   # MLPerf benchmark entry point
â”‚   â”œâ”€â”€ SUT_VLLM.py              # VLLM System Under Test
â”‚   â”œâ”€â”€ dataset.py               # CNN DailyMail data loader
â”‚   â”œâ”€â”€ loadgen/                 # Official MLPerf loadgen
â”‚   â””â”€â”€ run_official_benchmarks.sh
â”œâ”€â”€ config.yaml                  # Infrastructure configuration (user-customizable)
â”œâ”€â”€ config_manager.py            # Configuration management and script generation
â”œâ”€â”€ generate_visual_reports.py   # Visual report generator
â”œâ”€â”€ monitor_benchmarks.sh        # Auto-generated monitoring (customized for your infrastructure)
â”œâ”€â”€ run_benchmarks.sh           # Auto-generated benchmark runner (customized for your infrastructure)
â”œâ”€â”€ sample_visual_reports/        # Example visual reports
â”œâ”€â”€ SETUP_GUIDE.md               # Detailed setup guide for any infrastructure
â””â”€â”€ results/                     # Benchmark outputs (auto-generated)
```

## Benchmark Results

### Current Status
- **Implementation**: Official MLCommons reference
- **Model**: meta-llama/Llama-3.1-8B-Instruct  
- **Dataset**: CNN DailyMail (13,368 samples)
- **Scenario**: Server
- **Hardware**: 2x NVIDIA A30 24GB GPUs

### Visual Reports Generated
1. **Static Charts** (`mlperf_static_report.png`)
   - Performance comparison across runs
   - Latency distribution histograms
   - Throughput timeline analysis
   - ROUGE accuracy comparisons

2. **Interactive Dashboard** (`mlperf_interactive_dashboard.html`)
   - Web-based charts with hover details
   - Zoom, pan, and filter capabilities
   - Real-time data exploration

3. **Summary Report** (`README.md`)
   - Comprehensive analysis overview
   - Data source breakdown
   - Technical implementation details

## Automatic Report Generation

Visual reports are **automatically generated** when benchmarks complete:

```bash
# Enhanced monitoring with auto-reporting
./monitor_official_benchmarks.sh results

# Manual generation anytime
python3 generate_visual_reports.py results
```

Reports are saved to timestamped directories: `results/visual_reports_TIMESTAMP/`

## Technical Details

### MLPerf Compliance
- **Official loadgen**: `mlperf_loadgen.cpython-310-x86_64-linux-gnu.so`
- **Server scenario**: FirstTokenComplete callbacks with proper token reporting
- **Accuracy validation**: ROUGE scoring with 99% targets
- **Performance constraints**: Official MLPerf requirements

### Visualization Stack
- **Base Framework**: Official MLCommons trace analysis tools
- **Static Charts**: Matplotlib + Seaborn (publication quality)
- **Interactive Charts**: Plotly (web-based, interactive)
- **Data Processing**: Pandas + NumPy

### Hardware Configuration
- **GPUs**: 2x NVIDIA A30 24GB
- **Memory Optimization**: `gpu_memory_utilization=0.8, max_model_len=4096`
- **Distributed**: Kubernetes cluster (jw1 control, jw2/jw3 workers)

## Installation

```bash
# Clone repository
git clone https://github.com/jshim0978/MLPerf_local_test.git
cd MLPerf_local_test

# Setup environment
./setup_environment.sh

# Install visualization dependencies
pip install matplotlib seaborn plotly pandas numpy

# Download dataset (if needed)
cd official_mlperf
python3 download_cnndm.py
```

## Usage Examples

### Example 1: Single GPU Benchmark
```bash
cd official_mlperf
python3 main.py --scenario Server --model-path meta-llama/Llama-3.1-8B-Instruct
```

### Example 2: Multi-GPU Distributed
```bash
cd official_mlperf
./run_official_benchmarks.sh  # Runs on both jw2 and jw3
```

### Example 3: Live Monitoring with Auto-Reports
```bash
./monitor_official_benchmarks.sh watch
# Visual reports auto-generated when benchmarks complete
```

## Results Format

### Performance Results (`mlperf_log_summary.txt`)
```
Completed samples per second: 0.41
Completed tokens per second: 41.23
50.00 percentile latency (ns): 42000000000
99.00 percentile latency (ns): 96000000000
```

### Accuracy Results (`mlperf_log_accuracy.json`)
```json
{
  "metadata": {
    "rouge_scores": {
      "rouge1": 38.45,
      "rouge2": 15.67,
      "rougeL": 24.23
    },
    "accuracy_target": "99% of baseline ROUGE scores"
  }
}
```

## Contributing

This repository implements the **official MLCommons MLPerf inference reference** with enhanced visualization. To contribute:

1. Ensure changes maintain MLPerf compliance
2. Test with the complete CNN DailyMail dataset
3. Verify visual report generation functionality
4. Follow official MLCommons contribution guidelines

## License

This project uses the official MLCommons MLPerf inference code. See [LICENSE](LICENSE) for details.

---

**ðŸŽ¯ Results**: Professional MLPerf benchmarking with automatic visual reporting that transforms complex text outputs into intuitive, interactive dashboards.