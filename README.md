# MLPerf Llama-3.1-8B Benchmark

A streamlined, reproducible MLPerf inference benchmark for Meta's Llama-3.1-8B-Instruct model, designed for multi-GPU evaluation and team deployment.

## ğŸ¯ Quick Start

### 1. Setup Environment
```bash
# Clone repository
git clone https://github.com/jshim0978/MLPerf_local_test.git
cd MLPerf_local_test

# Configure environment
cp .env.example .env
# Edit .env with your HuggingFace token and node IPs

# Automated setup
./setup_environment.sh
```

### 2. Run Benchmark
```bash
# Set HuggingFace token
export HF_TOKEN=your_token_here

# Single node datacenter benchmark
python3 mlperf_datacenter_benchmark.py

# Multi-GPU coordinated benchmark (from controller node)
python3 run_datacenter_benchmark.py
```

### 3. View Results
```bash
# Generate comprehensive reports
python3 report_generator.py

# View latest results
cat reports/latest_summary.md
cat FINAL_BENCHMARK_SUMMARY.md
```

## ğŸ“Š Latest Performance Results

**Infrastructure:** 2Ã— NVIDIA A30 GPUs (jw2 + jw3)  
**Model:** Llama-3.1-8B-Instruct  
**Samples:** 20 server + 30 offline per GPU

| GPU | Server QPS | Offline QPS | Throughput | Accuracy | MLPerf Valid |
|-----|------------|-------------|------------|----------|--------------|
| **jw2** | 0.495 | 0.546 | 32.2 tok/sec | 100% | âœ… Server |
| **jw3** | 0.536 | 0.563 | 34.8 tok/sec | 100% | âœ… Server |
| **Total** | **1.031** | **1.109** | **67.0 tok/sec** | **100%** | **âœ… Both** |

## ğŸ—ï¸ Repository Structure

```
MLPerf_local_test/
â”œâ”€â”€ config.py                           # Environment-agnostic configuration
â”œâ”€â”€ mlperf_datacenter_benchmark.py      # Main benchmark (single GPU)
â”œâ”€â”€ run_datacenter_benchmark.py         # Multi-GPU coordinator
â”œâ”€â”€ report_generator.py                 # Automated report generation
â”œâ”€â”€ setup_environment.sh                # Environment setup
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .env.example                        # Configuration template
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ FINAL_BENCHMARK_SUMMARY.md          # Executive summary
â”œâ”€â”€ Dockerfile                          # Container support
â””â”€â”€ LICENSE                             # MIT License
```

## âš™ï¸ Configuration

### Environment Variables (.env)
```bash
HF_TOKEN=your_huggingface_token
MLPERF_USERNAME=your_username
JW2_IP=node2_ip_address
JW3_IP=node3_ip_address
MAX_TOKENS=64
SERVER_TARGET_QPS=1.0
```

### Hardware Requirements
- **GPUs:** NVIDIA A30/A100/H100 with 16GB+ VRAM
- **Memory:** 32GB+ system RAM
- **Storage:** 50GB+ free space
- **Network:** SSH access between nodes

## ğŸŒ Reproducibility Features

- **No hardcoded paths** - works on any infrastructure
- **Centralized configuration** - easy teammate deployment
- **Automated setup** - one script installation
- **Self-contained reports** - all outputs within project
- **Environment agnostic** - supports various node configurations

## ğŸ“ˆ MLPerf Compliance

- âœ… **MLPerf v5.0 Inference Datacenter** specifications
- âœ… **Server scenario validation** on both GPUs
- âœ… **99%+ accuracy requirement** (achieved 100%)
- âœ… **Latency constraints** met for server scenarios
- âœ… **Extended sample testing** (20-30 samples per scenario)

## ğŸš€ Team Deployment

Your teammates can deploy this anywhere by:

1. **Clone repository** to their infrastructure
2. **Copy .env.example to .env** and configure IPs/tokens
3. **Run ./setup_environment.sh** for automated setup
4. **Execute benchmarks** with single command
5. **Generate reports** with consistent formatting

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **MLCommons** for MLPerf benchmark framework
- **Meta** for Llama-3.1-8B model
- **HuggingFace** for model hosting and transformers
- **NVIDIA** for GPU compute infrastructure