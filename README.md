# üöÄ MLPerf Local Test - Multi-GPU Kubernetes Cluster

[![MLPerf](https://img.shields.io/badge/MLPerf-v5.0-blue.svg)](https://mlcommons.org/en/inference-datacenter-50/)
[![Kubernetes](https://img.shields.io/badge/kubernetes-1.28+-blue.svg)](https://kubernetes.io/)
[![NVIDIA](https://img.shields.io/badge/NVIDIA-A30-green.svg)](https://www.nvidia.com/en-us/data-center/a30/)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://python.org/)

A comprehensive MLPerf benchmark suite for testing GPU cluster performance with support for multiple accelerator types including NVIDIA GPUs and Furiosa NPUs.

## üèóÔ∏è **Cluster Architecture**

### **Current Setup**
| Node | Role | IP Address | Hardware | Status |
|------|------|------------|----------|---------|
| jw1 | Control Plane | 129.254.202.251 | CPU Only | ‚úÖ Active |
| jw2 | Worker | 129.254.202.252 | NVIDIA A30 | ‚úÖ Active |
| jw3 | Worker | 129.254.202.253 | NVIDIA A30 | ‚úÖ Active |

**Network**: Calico CNI | **Platform**: Ubuntu 22.04 | **Kubernetes**: v1.28+

---

## ‚ö° **Quick Start**

### **1. Environment Setup**
```bash
# Run automated setup (detects hardware automatically)
./scripts/setup-environment.sh

# For Kubernetes cluster setup
./scripts/setup-environment.sh --kubernetes

# Activate Python environment
source venv/bin/activate

# Set your HuggingFace token
export HF_TOKEN="your_token_here"
```

### **2. Run Benchmarks**

#### **Single/Multi-GPU Benchmarks**
```bash
# Single GPU benchmark (via coordinated mode)
python3 src/mlperf_benchmark.py --type coordinated --nodes jw2 --samples 10

# Multi-GPU coordinated benchmark
python3 src/mlperf_benchmark.py --type coordinated --nodes jw2,jw3 --samples 20

# Distributed multi-GPU benchmark
python3 src/mlperf_benchmark.py --type distributed --world-size 2
```

#### **MLPerf Datacenter Benchmark**
```bash
# Run MLPerf Inference v5.0 Datacenter benchmark
export SERVER_TARGET_QPS=0.5
export OFFLINE_TARGET_QPS=1.0
python3 src/mlperf_benchmark.py --type datacenter
```

### **3. View Results**
```bash
# Results automatically saved to results/latest/
ls results/latest/

# View comprehensive summary
cat results/20250721/comprehensive_benchmark_summary.md

# View automated reports
open reports/benchmark-execution-report.md
```

---

## üìä **Performance Results Summary**

### **üèÜ Latest Benchmark Results**

| Benchmark Type | jw2 Throughput | jw3 Throughput | Combined | Scaling Efficiency |
|----------------|----------------|----------------|----------|--------------------|
| **Coordinated Multi-GPU** | 0.98 samples/sec | 1.07 samples/sec | **2.05 samples/sec** | **2.05x** |
| **Distributed Multi-GPU** | 1.02 samples/sec | 1.09 samples/sec | **2.11 samples/sec** | **100%** |
| **Datacenter Server** | 0.50 QPS | 0.54 QPS | **1.03 QPS** | ‚úÖ **Valid** |

**Token Generation**: ~67-72 tokens/sec combined | **GPU Memory**: ~16GB per A30 | **Latency**: <3s

---

## üõ†Ô∏è **Supported Hardware**

### **NVIDIA GPUs**
- ‚úÖ **A30** (24GB) - Primary tested configuration
- ‚úÖ **H100** (80GB) - Configuration available
- ‚úÖ **Other NVIDIA GPUs** - Generic CUDA support

### **Furiosa NPUs**
- ‚úÖ **Warboy NPU** - Configuration and adapter available
- üîÑ **Driver Integration** - Setup scripts included

### **Generic Hardware**
- ‚úÖ **CPU-only** - Fallback support
- ‚úÖ **Mixed Environments** - Configurable hardware detection

---

## üìÅ **Repository Structure**

```
üì¶ MLPerf_local_test/
‚îú‚îÄ‚îÄ üìÑ README.md                    # This file
‚îú‚îÄ‚îÄ üìÑ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ üóÇÔ∏è src/                         # Source code
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mlperf_benchmark.py      # Main benchmark runner
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mlperf_datacenter_benchmark.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ report_generator.py      # Automated reporting
‚îÇ   ‚îî‚îÄ‚îÄ üóÇÔ∏è adapters/                # Hardware adapters
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ generic_adapter.py   # Generic hardware support
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ furiosa_adapter.py   # Furiosa NPU support
‚îú‚îÄ‚îÄ üóÇÔ∏è configs/                     # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ üóÇÔ∏è benchmark-configs/       # Hardware-specific configs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ nvidia-a30.yaml      # NVIDIA A30 optimized
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ furiosa-npu.yaml     # Furiosa NPU optimized
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ generic-config.yaml  # Generic template
‚îÇ   ‚îî‚îÄ‚îÄ üóÇÔ∏è kubernetes/              # K8s deployments
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ mlperf-job.yaml       # Benchmark job template
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ ntp-daemonset.yaml    # NTP synchronization
‚îú‚îÄ‚îÄ üóÇÔ∏è scripts/                     # Automation scripts
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ setup-environment.sh     # Environment setup
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ deploy.sh                # Deployment automation
‚îú‚îÄ‚îÄ üóÇÔ∏è docs/                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cluster-architecture.md  # Architecture details
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ setup-guide.md           # Detailed setup
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ troubleshooting.md       # Common issues
‚îú‚îÄ‚îÄ üóÇÔ∏è reports/                     # Generated reports
‚îî‚îÄ‚îÄ üóÇÔ∏è results/                     # Benchmark results
    ‚îî‚îÄ‚îÄ üóÇÔ∏è 20250721/                # Daily results
        ‚îî‚îÄ‚îÄ üìÑ comprehensive_benchmark_summary.md
```

---

## üéØ **Benchmark Types**

### **1. Coordinated Multi-GPU**
- **Purpose**: Test multi-GPU scaling efficiency
- **Execution**: Simultaneous execution across worker nodes
- **Metrics**: Throughput, latency, scaling efficiency
- **Usage**: `--type coordinated --nodes jw2,jw3`

### **2. Distributed Multi-GPU**
- **Purpose**: True distributed inference simulation
- **Execution**: Independent processes with coordination
- **Metrics**: Combined throughput, per-node performance
- **Usage**: `--type distributed --world-size 2`

### **3. MLPerf Datacenter**
- **Purpose**: MLPerf v5.0 compliance testing
- **Scenarios**: Server (QPS), Offline (throughput)
- **Validation**: Latency constraints, accuracy targets
- **Usage**: `--type datacenter`

---

## ‚öôÔ∏è **Configuration**

### **Environment Variables**
```bash
# Required
export HF_TOKEN="your_huggingface_token"

# Hardware Configuration
export HARDWARE_TYPE="nvidia-a30"          # Auto-detected
export CUDA_VISIBLE_DEVICES="0"            # GPU selection

# Performance Tuning
export SERVER_TARGET_QPS="0.5"             # Datacenter server QPS
export OFFLINE_TARGET_QPS="1.0"            # Datacenter offline QPS
export MAX_TOKENS="64"                     # Output token limit
export BATCH_SIZE="1"                      # Inference batch size
```

### **Hardware-Specific Configs**
```bash
# List available configurations
python3 src/mlperf_benchmark.py --list-configs

# Use specific hardware config
python3 src/mlperf_benchmark.py --config configs/benchmark-configs/nvidia-a30.yaml

# Create custom configuration
cp configs/benchmark-configs/generic-config.yaml configs/my-config.yaml
# Edit configs/my-config.yaml as needed
```

---

## üöÄ **Adding New Hardware**

### **1. Create Hardware Configuration**
```yaml
# configs/benchmark-configs/my-accelerator.yaml
hardware:
  type: "my-accelerator"
  model: "accelerator-v1"
  memory_gb: 32

benchmark:
  server_target_qps: 2.0
  # ... other settings

deployment:
  node_selector:
    accelerator: "my-accelerator"
  resources:
    limits:
      my-company.com/accelerator: 1
```

### **2. Create Hardware Adapter**
```python
# src/adapters/my_adapter.py
from adapters.generic_adapter import BaseHardwareAdapter

class MyAcceleratorAdapter(BaseHardwareAdapter):
    def initialize_device(self):
        # Initialize your accelerator
        pass
    
    def load_model(self, model_name):
        # Load model on your accelerator
        pass
    
    def run_inference(self, prompt, max_tokens):
        # Run inference
        pass
```

### **3. Update Environment Setup**
```bash
# Add to scripts/setup-environment.sh
case $HARDWARE_TYPE in
    my-accelerator)
        print_status "Setting up My Accelerator..."
        # Add installation steps
        ;;
esac
```

---

## üîß **Kubernetes Deployment**

### **Job-Based Execution**
```bash
# Deploy benchmark job
kubectl apply -f configs/kubernetes/mlperf-job.yaml

# Check status
kubectl get jobs
kubectl logs job/mlperf-benchmark

# Scale to multiple nodes
kubectl scale job mlperf-benchmark --replicas=2
```

### **Infrastructure Services**
```bash
# Deploy NTP synchronization
kubectl apply -f configs/kubernetes/ntp-daemonset.yaml

# Monitor cluster health
kubectl get nodes -o wide
kubectl top nodes
```

---

## üìä **Monitoring and Observability**

### **Real-time Monitoring**
```bash
# GPU utilization
watch nvidia-smi

# System resources
htop

# Kubernetes resources
kubectl top nodes
kubectl top pods
```

### **Performance Analysis**
- **Automated Reports**: Generated after each benchmark
- **Metrics Collection**: Throughput, latency, GPU utilization
- **Health Assessment**: Infrastructure status monitoring
- **Historical Tracking**: Results stored by date

---

## üîç **Troubleshooting**

### **Common Issues**

#### **GPU Memory Issues**
```bash
# Check GPU memory
nvidia-smi

# Reduce batch size
export BATCH_SIZE=1

# Clear GPU cache
python3 -c "import torch; torch.cuda.empty_cache()"
```

#### **Model Loading Issues**
```bash
# Check HuggingFace token
echo $HF_TOKEN

# Test model access
huggingface-cli login
```

#### **Network Issues**
```bash
# Test node connectivity
ping jw2
ping jw3

# Check SSH access
ssh jw2 "hostname"
ssh jw3 "hostname"
```

### **Debug Mode**
```bash
# Enable verbose logging
export PYTHONPATH=src:$PYTHONPATH
python3 src/mlperf_benchmark.py --type single --samples 1 --verbose
```

---

## üéØ **Performance Optimization**

### **A30 GPU Optimization**
- **Memory Usage**: ~16GB optimal utilization
- **Precision**: FP16 for memory efficiency
- **Batch Size**: 1 for latency optimization
- **Sequence Length**: 2048 max for balance

### **Multi-Node Optimization**
- **NTP Sync**: Critical for coordinated benchmarks
- **Network**: Calico CNI optimized for performance
- **Load Balancing**: Automatic distribution across nodes

---

## üìã **Next Steps & Roadmap**

### **Current Status** ‚úÖ
- ‚úÖ Multi-GPU scaling (2.05x efficiency)
- ‚úÖ MLPerf Datacenter compliance
- ‚úÖ Automated reporting
- ‚úÖ Hardware abstraction

### **Planned Improvements** üîÑ
- üîÑ Additional NPU support
- üîÑ Helm chart deployment
- üîÑ Advanced monitoring
- üîÑ Model optimization

### **Future Enhancements** üìã
- üìã Multi-cluster support
- üìã Custom model support
- üìã Performance profiling
- üìã CI/CD integration

---

## ü§ù **Contributing**

1. **Fork** the repository
2. **Create** a feature branch
3. **Test** on your hardware
4. **Submit** a pull request

### **Development Setup**
```bash
git clone https://github.com/jshim0978/MLPerf_local_test.git
cd MLPerf_local_test
./scripts/setup-environment.sh
source venv/bin/activate
```

---

## üìÑ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè **Acknowledgments**

- **MLCommons** for MLPerf specifications
- **NVIDIA** for A30 GPU support
- **Furiosa AI** for NPU integration
- **Kubernetes Community** for orchestration platform

---

<div align="center">

**üìä Benchmarked** | **üöÄ Optimized** | **üîß Production Ready**

*Built for high-performance AI inference at scale*

</div>