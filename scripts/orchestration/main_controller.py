#!/usr/bin/env python3
"""
MLPerf Main Controller Script
Orchestrates all 6 benchmark scripts with professional reporting
"""

import os
import sys
import subprocess
import json
import yaml
import argparse
import threading
import time
from datetime import datetime
from pathlib import Path

class MLPerfController:
    def __init__(self, output_dir="./results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.reports_dir = Path("reports")
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        self.scripts_dir = Path(__file__).parent.parent
        
        # Load configuration
        self.config = self.load_config()
        
        # Track benchmark results
        self.benchmark_results = {}
        
    def load_config(self):
        """Load configuration from config.yaml"""
        config_path = Path.cwd() / "config.yaml"
        if not config_path.exists():
            config_path = Path.cwd() / ".." / ".." / "config.yaml"
        
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def run_benchmark_script(self, script_name, args=None):
        """Run a specific benchmark script"""
        
        script_path = self.scripts_dir / "benchmarks" / script_name
        
        if not script_path.exists():
            print(f"âŒ Benchmark script not found: {script_path}")
            return False
        
        print(f"ğŸš€ Starting {script_name}...")
        
        cmd = [sys.executable, str(script_path)]
        if args:
            cmd.extend(args)
        
        cmd.extend(["--output-dir", str(self.output_dir), "--generate-report"])
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
            
            if result.returncode == 0:
                print(f"âœ… {script_name} completed successfully")
                self.benchmark_results[script_name] = {
                    "status": "success",
                    "output": result.stdout,
                    "timestamp": datetime.now().isoformat()
                }
                return True
            else:
                print(f"âŒ {script_name} failed")
                print(f"Error: {result.stderr}")
                self.benchmark_results[script_name] = {
                    "status": "failed", 
                    "error": result.stderr,
                    "timestamp": datetime.now().isoformat()
                }
                return False
                
        except subprocess.TimeoutExpired:
            print(f"â° {script_name} timed out")
            self.benchmark_results[script_name] = {
                "status": "timeout",
                "timestamp": datetime.now().isoformat()
            }
            return False
        except Exception as e:
            print(f"âŒ Error running {script_name}: {str(e)}")
            self.benchmark_results[script_name] = {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            return False
    
    def run_inference_benchmarks(self):
        """Run all 3 inference benchmark scripts"""
        
        print("ğŸ¯ Running Inference Benchmarks")
        print("=" * 50)
        
        # Single GPU inference on both nodes
        success_count = 0
        for node in ["jw2", "jw3"]:
            if self.run_benchmark_script("single_gpu_inference.py", ["--node", node]):
                success_count += 1
        
        # Multi-GPU inference on both nodes  
        for node in ["jw2", "jw3"]:
            if self.run_benchmark_script("multi_gpu_inference.py", ["--node", node]):
                success_count += 1
        
        # Distributed infrastructure inference
        if self.run_benchmark_script("distributed_infrastructure_inference.py"):
            success_count += 1
        
        print(f"ğŸ“Š Inference benchmarks completed: {success_count}/5 successful")
        return success_count == 5
    
    def run_training_benchmarks(self):
        """Run all 3 training benchmark frameworks (future implementation)"""
        
        print("ğŸ¯ Running Training Benchmark Frameworks")
        print("=" * 50)
        print("â³ Training benchmarks are prepared frameworks for future implementation")
        
        # Single GPU training frameworks
        success_count = 0
        for node in ["jw2", "jw3"]:
            if self.run_benchmark_script("single_gpu_training.py", ["--node", node]):
                success_count += 1
        
        # Multi-GPU training frameworks
        for node in ["jw2", "jw3"]:
            if self.run_benchmark_script("multi_gpu_training.py", ["--node", node]):
                success_count += 1
        
        # Distributed training framework
        if self.run_benchmark_script("distributed_training.py"):
            success_count += 1
        
        print(f"ğŸ“Š Training frameworks validated: {success_count}/5 successful")
        return success_count == 5
    
    def run_single_gpu_benchmarks(self):
        """Run only single GPU benchmarks"""
        
        print("ğŸ¯ Running Single GPU Benchmarks")
        print("=" * 30)
        
        success_count = 0
        for node in ["jw2", "jw3"]:
            if self.run_benchmark_script("single_gpu_inference.py", ["--node", node]):
                success_count += 1
        
        print(f"ğŸ“Š Single GPU benchmarks completed: {success_count}/2 successful") 
        return success_count == 2
    
    def run_distributed_benchmarks(self):
        """Run only distributed benchmarks"""
        
        print("ğŸ¯ Running Distributed Benchmarks")
        print("=" * 35)
        
        success = self.run_benchmark_script("distributed_infrastructure_inference.py")
        
        print(f"ğŸ“Š Distributed benchmarks completed: {'1/1 successful' if success else '0/1 failed'}")
        return success
    
    def generate_comprehensive_report(self):
        """Generate comprehensive markdown report of all benchmarks"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.reports_dir / f"{timestamp}_comprehensive_benchmark_report.md"
        
        # Count successful benchmarks
        successful = len([r for r in self.benchmark_results.values() if r['status'] == 'success'])
        total = len(self.benchmark_results)
        
        report_content = f"""# MLPerf Comprehensive Benchmark Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Controller:** Main MLPerf Orchestration System  
**Success Rate:** {successful}/{total} benchmarks successful ({successful/total*100:.1f}%)

## Executive Summary

### Infrastructure Overview
- **Nodes:** {[node['name'] for node in self.config['infrastructure']['gpu_nodes']]}
- **GPU Type:** NVIDIA A30 (24GB)
- **Model:** Llama-3.1-8B-Instruct
- **Dataset:** CNN DailyMail (13,368 samples)
- **Scenario:** Server

### Benchmark Results Overview

| Benchmark Type | Status | Execution Time |
|----------------|--------|----------------|
"""
        
        for script_name, result in self.benchmark_results.items():
            status_icon = "âœ…" if result['status'] == 'success' else "âŒ"
            benchmark_name = script_name.replace('.py', '').replace('_', ' ').title()
            report_content += f"| {benchmark_name} | {status_icon} {result['status'].title()} | {result['timestamp']} |\n"
        
        report_content += f"""

## Detailed Results

### Infrastructure Configuration
```yaml
{yaml.dump(self.config['infrastructure'], default_flow_style=False)}
```

### Individual Benchmark Details

"""
        
        for script_name, result in self.benchmark_results.items():
            benchmark_name = script_name.replace('.py', '').replace('_', ' ').title()
            report_content += f"""#### {benchmark_name}
- **Status:** {result['status'].title()}
- **Timestamp:** {result['timestamp']}
"""
            if result['status'] == 'success' and 'output' in result:
                report_content += f"- **Output:** Success\n"
            elif 'error' in result:
                report_content += f"- **Error:** {result['error'][:200]}...\n"
            
            report_content += "\n"
        
        # MLPerf Features
        report_content += """## MLPerf Compliance Features

- âœ… **Official MLCommons Implementation** - Using genuine MLPerf reference code
- âœ… **Full Dataset Compliance** - Complete CNN DailyMail dataset (13,368 samples)  
- âœ… **Production Inference Engine** - VLLM optimization for real-world performance
- âœ… **Official Scoring Metrics** - ROUGE accuracy validation
- âœ… **Server Scenario Compliance** - FirstTokenComplete callbacks
- âœ… **Professional Reporting** - Enterprise-grade result documentation

## Next Steps

1. **Performance Analysis:** Compare results against MLPerf baselines
2. **Optimization:** Identify performance improvement opportunities  
3. **Training Implementation:** Develop actual training benchmark implementations
4. **Monitoring Enhancement:** Add real-time performance dashboards

---
*Generated by MLPerf Professional Benchmarking System*
"""
        
        # Save report
        with open(report_file, 'w') as f:
            f.write(report_content)
        
        print(f"ğŸ“Š Comprehensive report generated: {report_file}")
        return report_file
    
    def show_live_status(self):
        """Show live status of currently running benchmarks"""
        
        print("ğŸ“Š Live Benchmark Status Monitor")
        print("=" * 40)
        
        # Check if any benchmarks are currently running
        active_benchmarks = []
        for node_data in self.config['infrastructure']['gpu_nodes']:
            node_name = node_data['name']
            node_config = {'ip': node_data['ip'], 'username': node_data['ssh_user']}
            try:
                # Check for running processes
                ssh_cmd = ["ssh", f"{node_config['username']}@{node_config['ip']}", 
                          "ps aux | grep -q 'python3.*main.py' && echo 'RUNNING' || echo 'IDLE'"]
                result = subprocess.run(ssh_cmd, capture_output=True, text=True, timeout=10)
                
                status = result.stdout.strip()
                print(f"ğŸ–¥ï¸  {node_name} ({node_config['ip']}): {status}")
                
                if status == "RUNNING":
                    active_benchmarks.append(node_name)
                    
            except Exception as e:
                print(f"ğŸ–¥ï¸  {node_name}: Connection Error")
        
        if active_benchmarks:
            print(f"\nâš¡ Active benchmarks detected on: {', '.join(active_benchmarks)}")
            print("ğŸ’¡ Use existing monitor: ./monitor_benchmarks.sh watch")
        else:
            print("\nğŸ’¤ No active benchmarks detected")
        
        return len(active_benchmarks) > 0

def main():
    parser = argparse.ArgumentParser(description="MLPerf Main Controller - Orchestrate All Benchmarks")
    
    # Benchmark selection
    parser.add_argument("--run-all", action="store_true",
                       help="Run all 6 benchmark scripts (3 inference + 3 training)")
    parser.add_argument("--run-inference", action="store_true", 
                       help="Run only the 3 inference benchmarks")
    parser.add_argument("--run-training", action="store_true",
                       help="Run only the 3 training framework scripts")
    parser.add_argument("--run-single-gpu", action="store_true",
                       help="Run only single GPU benchmarks")
    parser.add_argument("--run-distributed", action="store_true", 
                       help="Run only distributed benchmarks")
    
    # Options
    parser.add_argument("--output-dir", default="./results",
                       help="Output directory for all benchmark results")
    parser.add_argument("--generate-reports", action="store_true",
                       help="Generate comprehensive report after completion")
    parser.add_argument("--status", action="store_true",
                       help="Show live status of running benchmarks")
    
    args = parser.parse_args()
    
    print("ğŸš€ MLPerf Main Controller")
    print("=" * 50)
    print("Professional MLPerf Benchmarking Orchestration System")
    print("")
    
    controller = MLPerfController(args.output_dir)
    
    # Show status if requested
    if args.status:
        controller.show_live_status()
        return
    
    # Determine which benchmarks to run
    benchmarks_run = False
    
    if args.run_all:
        print("ğŸ¯ Running ALL benchmarks (inference + training frameworks)")
        success1 = controller.run_inference_benchmarks()
        success2 = controller.run_training_benchmarks() 
        benchmarks_run = success1 or success2
        
    elif args.run_inference:
        success = controller.run_inference_benchmarks()
        benchmarks_run = success
        
    elif args.run_training:
        success = controller.run_training_benchmarks()
        benchmarks_run = success
        
    elif args.run_single_gpu:
        success = controller.run_single_gpu_benchmarks()
        benchmarks_run = success
        
    elif args.run_distributed:
        success = controller.run_distributed_benchmarks()
        benchmarks_run = success
    
    else:
        print("â“ No benchmark option specified. Use --help to see available options.")
        print("\nQuick start options:")
        print("  --run-inference     Run inference benchmarks")
        print("  --run-all          Run all benchmarks")
        print("  --status           Show live benchmark status")
        return
    
    # Generate comprehensive report if requested and benchmarks were run
    if args.generate_reports and benchmarks_run and controller.benchmark_results:
        print("\n" + "="*60)
        print("ğŸ“Š GENERATING COMPREHENSIVE REPORT")
        print("="*60)
        controller.generate_comprehensive_report()
    
    # Final summary
    if benchmarks_run and controller.benchmark_results:
        successful = len([r for r in controller.benchmark_results.values() if r['status'] == 'success'])
        total = len(controller.benchmark_results)
        
        print(f"\nğŸ‰ BENCHMARK ORCHESTRATION COMPLETED")
        print(f"ğŸ“Š Final Results: {successful}/{total} benchmarks successful")
        print(f"ğŸ“ Results saved to: {controller.output_dir}")
        print(f"ğŸ“‹ Reports available in: {controller.reports_dir}")

if __name__ == "__main__":
    main()