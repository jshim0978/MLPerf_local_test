apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-test-single-gpu-jw2-server
  labels:
    app: mlperf-test-benchmark
    node: jw2
    type: single-gpu
    scenario: server
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: mlperf-test-benchmark
        node: jw2
    spec:
      nodeSelector:
        kubernetes.io/hostname: jw2
      restartPolicy: Never
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - name: mlperf-llama
        image: nvcr.io/nvidia/pytorch:24.07-py3
        command: ["/bin/bash"]
        args:
        - -c
        - |
          set -e
          echo "ðŸ§ª Starting TEST MLPerf Llama-3.1-8B Server Scenario on jw2"
          echo "============================================================"
          echo "ðŸ“Š TEST MODE: 20 samples only (should complete in ~3-5 minutes)"
          
          # Install dependencies
          pip install transformers==4.46.2 nltk==3.8.1 evaluate==0.4.0 absl-py==1.4.0 rouge-score==0.1.2 sentencepiece==0.2.0 accelerate==0.21.0 vllm==0.6.3 pybind11==2.10.4
          
          # Clone MLCommons inference repo
          git clone --depth 1 https://github.com/mlcommons/inference.git
          cd inference/loadgen && pip install -e .
          cd ../language/llama3.1-8b
          
          # Download dataset
          curl -o cnn_eval.json https://huggingface.co/datasets/MLCommons/mlperf-inference-cnn-dailymail/resolve/main/cnn_eval.json
          
          # Run TEST MLPerf benchmark - Server scenario with 20 samples
          echo "Running TEST Server scenario benchmark (20 samples)..."
          python3 -u main.py \
            --scenario Server \
            --model-path meta-llama/Llama-3.1-8B-Instruct \
            --batch-size 1 \
            --dtype float16 \
            --user-conf user.conf \
            --total-sample-count 20 \
            --dataset-path cnn_eval.json \
            --output-log-dir /shared/results/jw2_test_server_results \
            --vllm
          
          echo "âœ… TEST Server benchmark completed successfully on jw2"
          
          # Copy results to shared location
          mkdir -p /shared/results/jw2_test_server_results
          cp -r output-logs/* /shared/results/jw2_test_server_results/ || true
          
          echo "ðŸ“Š Test results saved to /shared/results/jw2_test_server_results"
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "4"
          limits:
            nvidia.com/gpu: 1
            memory: "32Gi"
            cpu: "8"
        volumeMounts:
        - name: shared-results
          mountPath: /shared
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: NODE_NAME
          value: jw2
        - name: HF_TOKEN
          value: "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"
      volumes:
      - name: shared-results
        hostPath:
          path: /home/jungwooshim/results
          type: DirectoryOrCreate