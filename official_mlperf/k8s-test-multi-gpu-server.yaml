apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-test-multi-gpu-server
  labels:
    app: mlperf-test-benchmark
    type: multi-gpu-distributed
    scenario: server
spec:
  parallelism: 2
  completions: 2
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: mlperf-test-benchmark
        type: multi-gpu-distributed
    spec:
      restartPolicy: Never
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: ["mlperf-test-benchmark"]
            topologyKey: kubernetes.io/hostname
      containers:
      - name: mlperf-llama-distributed
        image: nvcr.io/nvidia/pytorch:24.07-py3
        command: ["/bin/bash"]
        args:
        - -c
        - |
          set -e
          echo "ðŸ§ª Starting TEST MLPerf Llama-3.1-8B Multi-GPU Server Scenario"
          echo "==============================================================="
          echo "ðŸ“Š TEST MODE: 20 samples only (should complete in ~3-5 minutes)"
          
          # Install dependencies
          pip install transformers==4.46.2 nltk==3.8.1 evaluate==0.4.0 absl-py==1.4.0 rouge-score==0.1.2 sentencepiece==0.2.0 accelerate==0.21.0 vllm==0.6.3 pybind11==2.10.4
          
          # Clone MLCommons inference repo
          git clone --depth 1 https://github.com/mlcommons/inference.git
          cd inference/loadgen && pip install -e .
          cd ../language/llama3.1-8b
          
          # Download dataset
          curl -o cnn_eval.json https://huggingface.co/datasets/MLCommons/mlperf-inference-cnn-dailymail/resolve/main/cnn_eval.json
          
          # Run TEST MLPerf benchmark with tensor parallelism across 2 GPUs
          echo "Running TEST Multi-GPU Server scenario benchmark (20 samples)..."
          python3 -u main.py \
            --scenario Server \
            --model-path meta-llama/Llama-3.1-8B-Instruct \
            --batch-size 2 \
            --dtype float16 \
            --user-conf user.conf \
            --total-sample-count 20 \
            --dataset-path cnn_eval.json \
            --output-log-dir /shared/results/multi_gpu_test_server_results \
            --tensor-parallel-size 2 \
            --vllm
          
          echo "âœ… TEST Multi-GPU Server benchmark completed successfully"
          
          # Copy results to shared location
          mkdir -p /shared/results/multi_gpu_test_server_results
          cp -r output-logs/* /shared/results/multi_gpu_test_server_results/ || true
          
          echo "ðŸ“Š Test results saved to /shared/results/multi_gpu_test_server_results"
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "4"
          limits:
            nvidia.com/gpu: 1
            memory: "32Gi"
            cpu: "8"
        volumeMounts:
        - name: shared-results
          mountPath: /shared
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: HF_TOKEN
          value: "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"
      volumes:
      - name: shared-results
        hostPath:
          path: /home/jungwooshim/results
          type: DirectoryOrCreate