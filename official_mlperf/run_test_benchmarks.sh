#!/bin/bash
#
# MLPerf Test Benchmark Runner - Quick Tests with 20 samples
# Online scenarios only (Server) on jw2, jw3, and multi-GPU
#

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
RESULTS_DIR="/home/jungwooshim/results"

echo "🧪 MLPerf Test Benchmark Runner (Server Scenario)"
echo "================================================="
echo "Using 20 samples for quick testing (~3-5 minutes each)"
echo "Testing online scenarios only:"
echo "  • Single GPU jw2 - Server scenario" 
echo "  • Single GPU jw3 - Server scenario"
echo "  • Multi-GPU distributed - Server scenario"
echo ""

# Create results directory
mkdir -p "$RESULTS_DIR"

function run_test_single_gpu_jw2() {
    echo "🧪 Running TEST Single GPU Server Benchmark on jw2..."
    kubectl delete job mlperf-test-single-gpu-jw2-server --ignore-not-found=true
    kubectl apply -f "$SCRIPT_DIR/k8s-test-single-gpu-jw2-server.yaml"
    
    echo "⏳ Waiting for jw2 test benchmark to complete..."
    kubectl wait --for=condition=complete --timeout=900s job/mlperf-test-single-gpu-jw2-server
    
    echo "✅ jw2 test benchmark completed!"
    kubectl logs job/mlperf-test-single-gpu-jw2-server | tail -20
}

function run_test_single_gpu_jw3() {
    echo "🧪 Running TEST Single GPU Server Benchmark on jw3..."
    kubectl delete job mlperf-test-single-gpu-jw3-server --ignore-not-found=true
    kubectl apply -f "$SCRIPT_DIR/k8s-test-single-gpu-jw3-server.yaml"
    
    echo "⏳ Waiting for jw3 test benchmark to complete..."
    kubectl wait --for=condition=complete --timeout=900s job/mlperf-test-single-gpu-jw3-server
    
    echo "✅ jw3 test benchmark completed!"
    kubectl logs job/mlperf-test-single-gpu-jw3-server | tail -20
}

function run_test_multi_gpu_distributed() {
    echo "🧪 Running TEST Multi-GPU Distributed Server Benchmark..."
    kubectl delete job mlperf-test-multi-gpu-server --ignore-not-found=true
    kubectl apply -f "$SCRIPT_DIR/k8s-test-multi-gpu-server.yaml"
    
    echo "⏳ Waiting for multi-GPU test benchmark to complete..."
    kubectl wait --for=condition=complete --timeout=900s job/mlperf-test-multi-gpu-server
    
    echo "✅ Multi-GPU test benchmark completed!"
    kubectl logs job/mlperf-test-multi-gpu-server | tail -20
}

function generate_test_report() {
    echo "📊 Generating Test MLPerf Report..."
    
    # Create report using test results
    cat > "$RESULTS_DIR/test_mlperf_report.md" << EOF
# MLPerf Test Results - Server Scenario (20 samples)

Generated on: $(date)
Implementation: Official MLCommons Reference
Dataset: CNN DailyMail (20 test samples)
Scenario: Server (Online)

## System Configuration
- jw1 (control plane): No GPU
- jw2 (worker): 1x NVIDIA A30 GPU  
- jw3 (worker): 1x NVIDIA A30 GPU

## Test Results (20 samples each)

### Single GPU - jw2 (Server Scenario)
$(ls $RESULTS_DIR/jw2_test_server_results/ 2>/dev/null | head -10 || echo "Results pending...")

### Single GPU - jw3 (Server Scenario)  
$(ls $RESULTS_DIR/jw3_test_server_results/ 2>/dev/null | head -10 || echo "Results pending...")

### Multi-GPU Distributed (Server Scenario)
$(ls $RESULTS_DIR/multi_gpu_test_server_results/ 2>/dev/null | head -10 || echo "Results pending...")

## MLPerf Compliance (Test Mode)
- ✅ Official MLCommons loadgen
- ✅ VLLM optimization
- ✅ CNN DailyMail dataset (20 samples for testing)
- ✅ Server scenario (online inference)
- ✅ Proper token counting and reporting
- ✅ FirstTokenComplete callbacks

Generated by Official MLPerf Test Implementation
EOF
    
    echo "📋 Test report saved to: $RESULTS_DIR/test_mlperf_report.md"
}

function main() {
    case "${1:-all}" in
        "jw2")
            run_test_single_gpu_jw2
            ;;
        "jw3")
            run_test_single_gpu_jw3
            ;;
        "multi-gpu")
            run_test_multi_gpu_distributed
            ;;
        "all")
            echo "🏃 Running all test MLPerf benchmarks..."
            run_test_single_gpu_jw2
            run_test_single_gpu_jw3  
            run_test_multi_gpu_distributed
            generate_test_report
            ;;
        "report")
            generate_test_report
            ;;
        *)
            echo "Usage: $0 [jw2|jw3|multi-gpu|all|report]"
            exit 1
            ;;
    esac
    
    echo ""
    echo "🎉 Test MLPerf benchmarks completed!"
    echo "📁 Results available in: $RESULTS_DIR"
    echo "💡 Each test used only 20 samples for quick validation"
}

main "$@"