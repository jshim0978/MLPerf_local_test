# MLPerf Benchmark Configuration
# =================================
# Configure this file for your infrastructure before running benchmarks

# Infrastructure Configuration
infrastructure:
  # Deployment type: "kubernetes", "docker", "local", "ssh"
  deployment_type: "ssh"
  
  # GPU nodes configuration
  gpu_nodes:
    - name: "jw2"                  # Your actual GPU node
      ip: "129.254.202.252"        # Your actual jw2 IP
      ssh_user: "jungwooshim"      # Your actual SSH username
      gpu_type: "NVIDIA A30"       # Your GPU model
      gpu_memory: "24GB"           # GPU memory
      
    - name: "jw3"                  # Your actual GPU node  
      ip: "129.254.202.253"        # Your actual jw3 IP
      ssh_user: "jungwooshim"      # Your actual SSH username
      gpu_type: "NVIDIA A30"       # Your GPU model
      gpu_memory: "24GB"           # GPU memory

# Benchmark Configuration
benchmark:
  model: "meta-llama/Llama-3.1-8B-Instruct"
  scenario: "Server"              # Server, Offline, SingleStream, MultiStream
  dataset: 
    name: "CNN DailyMail"
    total_samples: 13368
    file_path: "cnn_eval.json"
  
  # Performance targets (adjust based on your hardware)
  performance:
    target_qps: 0.5
    min_duration: 120000          # milliseconds
    max_duration: 600000          # milliseconds

# Directory Configuration  
directories:
  # Remote directory where MLPerf is installed on GPU nodes
  remote_mlperf_dir: "~/official_mlperf/inference/language/llama3.1-8b"
  
  # Local directory to store results
  local_results_dir: "./results"
  
  # Directory for visual reports
  visual_reports_dir: "./results/visual_reports"

# Monitoring Configuration
monitoring:
  check_interval: 60             # seconds between progress checks
  auto_generate_reports: true    # automatically generate visual reports
  save_logs: true               # save benchmark logs locally
  
# Optional: Kubernetes Configuration (if deployment_type is "kubernetes")
kubernetes:
  namespace: "mlperf"
  config_file: "~/.kube/config"
  gpu_resource: "nvidia.com/gpu"

# Optional: Docker Configuration (if deployment_type is "docker")
docker:
  image: "mlperf-inference:latest"
  runtime: "nvidia"
  volumes:
    - "/data:/data"
    - "/models:/models"

# Optional: Local Configuration (if deployment_type is "local")
local:
  python_env: "conda"            # conda, venv, system
  env_name: "mlperf"             # environment name if using conda/venv