# Single Node Configuration Template
# For standalone servers or development environments

model:
  name: "meta-llama/Llama-3.1-8B-Instruct"
  max_tokens: 64
  batch_size: 1
  cache_dir: "./cache"

hardware:
  accelerator_type: "auto"  # Will auto-detect: nvidia, furiosa, amd, intel, cpu
  device_count: 1
  memory_limit: "16GB"

scenarios:
  server:
    target_qps: 1.0
    latency_constraint_ms: 1000
    duration_ms: 60000  # 1 minute for quick testing
    min_queries: 50
  
  offline:
    target_qps: 5.0
    duration_ms: 60000
    min_queries: 50

deployment:
  type: "standalone"
  results_dir: "./results"
  log_level: "INFO"

# Environment variables for standalone deployment
environment:
  HF_TOKEN: "${HF_TOKEN}"  # Set via export HF_TOKEN=your_token
  CUDA_VISIBLE_DEVICES: "0"  # GPU selection
  OMP_NUM_THREADS: "8"  # CPU threading
  TOKENIZERS_PARALLELISM: "false"  # Avoid warnings