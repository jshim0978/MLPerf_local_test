# Default MLPerf run settings (edit as needed)
model: llama3.1-8b
device: a30
scenario: _all-scenarios  # or Offline/Server/SingleStream
framework: vllm
precision: float16
gpu_memory_utilization: 0.95
max_model_len: 8192
max_num_batched_tokens: 8192
max_num_seqs: 256
accuracy: true
extra_args: ""
