# NVIDIA Multi-GPU Configuration
# For systems with multiple NVIDIA GPUs

apiVersion: v1
kind: ConfigMap
metadata:
  name: nvidia-multi-gpu-config
  namespace: mlperf
data:
  config.yaml: |
    # MLPerf Benchmark Configuration for NVIDIA Multi-GPU
    
    model:
      name: "meta-llama/Llama-3.1-8B-Instruct"
      max_tokens: 64
      batch_size: 1
      torch_dtype: "float16"
      device_map: "auto"
    
    hardware:
      accelerator_type: "nvidia"
      device_count: 4  # Adjust based on available GPUs
      cuda_version: "12.1"
      memory_per_gpu: "24GB"  # A100, adjust for your GPUs
    
    scenarios:
      server:
        target_qps: 4.0  # 1.0 per GPU
        latency_constraint_ms: 1000
        duration_ms: 120000  # 2 minutes
        min_queries: 200
      
      offline:
        target_qps: 40.0  # 10.0 per GPU
        duration_ms: 120000
        min_queries: 200
    
    deployment:
      type: "kubernetes"
      replicas: 1
      resources:
        requests:
          cpu: "8"
          memory: "32Gi"
          nvidia.com/gpu: "4"
        limits:
          cpu: "16"
          memory: "64Gi"
          nvidia.com/gpu: "4"

---
apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-nvidia-multi-gpu
  namespace: mlperf
spec:
  template:
    spec:
      containers:
      - name: mlperf-benchmark
        image: mlperf-benchmark:nvidia
        env:
        - name: ACCELERATOR_TYPE
          value: "nvidia"
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: mlperf-secrets
              key: hf-token
        - name: SERVER_TARGET_QPS
          value: "4.0"
        - name: OFFLINE_TARGET_QPS
          value: "40.0"
        volumeMounts:
        - name: config
          mountPath: /app/configs
        - name: results
          mountPath: /app/results
        - name: cache
          mountPath: /app/cache
        resources:
          requests:
            cpu: "8"
            memory: "32Gi"
            nvidia.com/gpu: "4"
          limits:
            cpu: "16" 
            memory: "64Gi"
            nvidia.com/gpu: "4"
      volumes:
      - name: config
        configMap:
          name: nvidia-multi-gpu-config
      - name: results
        persistentVolumeClaim:
          claimName: mlperf-results
      - name: cache
        persistentVolumeClaim:
          claimName: mlperf-cache
      restartPolicy: Never
  backoffLimit: 2