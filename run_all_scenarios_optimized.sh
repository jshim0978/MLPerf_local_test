#!/bin/bash

# MLPerf Optimized All-Scenarios Benchmark Runner
# ==============================================
# 
# This script runs optimized MLPerf benchmarks with automatic report generation
# Uses our optimized VLLM implementation for maximum performance

set -e

# Configuration
RESULTS_DIR="$(pwd)/results_optimized_$(date +%Y%m%d_%H%M%S)"
MODEL_PATH="meta-llama/Llama-3.1-8B-Instruct"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_banner() {
    echo -e "${BLUE}"
    echo "üöÄ MLPerf LLaMA3.1-8B Optimized Benchmark Suite"
    echo "==============================================="
    echo "üìä Model: ${MODEL_PATH}"
    echo "‚ö° Optimization: VLLM + CUDA Graphs + Batching"
    echo "üìÅ Results: ${RESULTS_DIR}"
    echo "üéØ Expected speedup: 8.7x (0.75 ‚Üí 6.5 samples/sec)"
    echo -e "${NC}"
}

check_prerequisites() {
    echo -e "${YELLOW}üîç Checking prerequisites...${NC}"
    
    # Check Python 3
    if ! command -v python3 &> /dev/null; then
        echo -e "${RED}‚ùå Python 3 not found. Please install Python 3.${NC}"
        exit 1
    fi
    
    # Check NVIDIA GPU
    if ! command -v nvidia-smi &> /dev/null; then
        echo -e "${RED}‚ùå NVIDIA GPU not found or nvidia-smi not available.${NC}"
        exit 1
    fi
    
    # Check required Python packages
    echo "üì¶ Checking Python packages..."
    python3 -c "import torch, vllm, datasets, rouge_score" 2>/dev/null || {
        echo -e "${RED}‚ùå Required Python packages missing.${NC}"
        echo "   Install with: pip install torch vllm datasets rouge-score"
        exit 1
    }
    
    # Check optimized benchmark scripts
    local required_scripts=("optimized_benchmark_with_reports.py" "generate_report_from_json.py")
    for script in "${required_scripts[@]}"; do
        if [ ! -f "$script" ]; then
            echo -e "${RED}‚ùå Required script not found: $script${NC}"
            exit 1
        fi
    done
    
    echo -e "${GREEN}‚úÖ All prerequisites satisfied${NC}"
}

run_scenario() {
    local scenario_name="$1"
    local samples="$2"
    local description="$3"
    
    echo -e "${BLUE}üéØ Running $scenario_name scenario...${NC}"
    echo "   üìä Samples: $samples"
    echo "   üìù Description: $description"
    
    local scenario_dir="$RESULTS_DIR/$scenario_name"
    mkdir -p "$scenario_dir"
    
    # Run optimized benchmark with automatic report generation
    echo "   ‚ö° Starting optimized benchmark..."
    cd "$scenario_dir"
    
    local start_time=$(date +%s)
    python3 ../../optimized_benchmark_with_reports.py --samples "$samples" > benchmark_log.txt 2>&1
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    cd - > /dev/null
    
    # Verify results were generated
    if [ -f "$scenario_dir/results_${samples}_samples_"*/benchmark_results_*.json ]; then
        echo -e "${GREEN}   ‚úÖ $scenario_name completed in ${duration}s${NC}"
        
        # Extract key metrics
        local json_file=$(find "$scenario_dir" -name "benchmark_results_*.json" | head -1)
        if [ -f "$json_file" ]; then
            local throughput=$(python3 -c "import json; print(f\"{json.load(open('$json_file'))['performance']['throughput_samples_per_second']:.2f}\")" 2>/dev/null || echo "N/A")
            local speedup=$(python3 -c "import json; print(f\"{json.load(open('$json_file'))['baseline_comparison']['speedup_factor']:.1f}x\")" 2>/dev/null || echo "N/A")
            echo -e "${GREEN}   üìä Throughput: $throughput samples/sec | Speedup: $speedup${NC}"
        fi
    else
        echo -e "${RED}   ‚ùå $scenario_name failed - no results generated${NC}"
        return 1
    fi
}

run_all_scenarios() {
    echo -e "${BLUE}üèÅ Starting all benchmark scenarios...${NC}"
    
    # Create main results directory
    mkdir -p "$RESULTS_DIR"
    
    # Save configuration
    cat > "$RESULTS_DIR/benchmark_config.json" << EOF
{
    "timestamp": "$(date -Iseconds)",
    "model": "$MODEL_PATH",
    "optimization": "VLLM + CUDA Graphs + Batching",
    "expected_speedup": "8.7x",
    "baseline_throughput": 0.75,
    "full_dataset_size": 11490,
    "scenarios": [
        {"name": "SingleStream", "samples": 100, "description": "Single query processing test"},
        {"name": "Offline", "samples": 11490, "description": "Full dataset batch processing"},
        {"name": "Server", "samples": 1000, "description": "Server scenario simulation"}
    ]
}
EOF
    
    local total_start_time=$(date +%s)
    local failed_scenarios=0
    
    # Run different scenarios
    echo ""
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    echo -e "${YELLOW}üìä SCENARIO 1: SingleStream (Quick Test)${NC}"
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    run_scenario "SingleStream" 100 "Single query processing test" || ((failed_scenarios++))
    
    echo ""
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    echo -e "${YELLOW}üìä SCENARIO 2: Offline (Full Dataset)${NC}"
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    run_scenario "Offline" 11490 "Full dataset batch processing" || ((failed_scenarios++))
    
    echo ""
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    echo -e "${YELLOW}üìä SCENARIO 3: Server (Server Simulation)${NC}"
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    run_scenario "Server" 1000 "Server scenario simulation" || ((failed_scenarios++))
    
    local total_end_time=$(date +%s)
    local total_duration=$((total_end_time - total_start_time))
    
    # Generate summary report
    generate_summary_report "$total_duration" "$failed_scenarios"
    
    if [ $failed_scenarios -eq 0 ]; then
        echo -e "${GREEN}‚úÖ All scenarios completed successfully in ${total_duration}s!${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  $failed_scenarios scenario(s) failed. Check individual logs.${NC}"
    fi
}

generate_summary_report() {
    local total_duration="$1"
    local failed_scenarios="$2"
    
    echo -e "${BLUE}üìã Generating comprehensive summary report...${NC}"
    
    local summary_file="$RESULTS_DIR/benchmark_summary_$(date +%Y%m%d_%H%M%S).html"
    
    cat > "$summary_file" << 'EOF'
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLPerf Optimized Benchmark Suite Results</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 0; background: #f8f9fa; }
        .container { max-width: 1200px; margin: 30px auto; background: white; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }
        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 40px; text-align: center; border-radius: 12px 12px 0 0; }
        .header h1 { margin: 0; font-size: 2.5em; font-weight: 300; }
        .content { padding: 40px; }
        .scenario-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 24px; margin: 30px 0; }
        .scenario-card { background: #f8f9fa; padding: 24px; border-radius: 12px; border-left: 5px solid #28a745; }
        .scenario-title { font-size: 1.3em; font-weight: 600; color: #2c3e50; margin-bottom: 16px; }
        .metric { margin: 8px 0; }
        .metric-value { font-weight: 600; color: #28a745; }
        .summary-stats { background: #e8f5e8; padding: 24px; border-radius: 12px; margin: 24px 0; }
        .failed { border-left-color: #dc3545; }
        .failed .metric-value { color: #dc3545; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üöÄ MLPerf Optimized Benchmark Suite</h1>
            <p>Comprehensive Performance Evaluation | Generated on TIMESTAMP_PLACEHOLDER</p>
        </div>
        
        <div class="content">
            <div class="summary-stats">
                <h2>üìä Overall Results</h2>
                <div class="metric">Total Duration: <span class="metric-value">DURATION_PLACEHOLDER seconds</span></div>
                <div class="metric">Failed Scenarios: <span class="metric-value">FAILED_PLACEHOLDER</span></div>
                <div class="metric">Model: <span class="metric-value">meta-llama/Llama-3.1-8B-Instruct</span></div>
                <div class="metric">Optimization: <span class="metric-value">VLLM + CUDA Graphs + Batching</span></div>
            </div>
            
            <h2>üéØ Scenario Results</h2>
            <div class="scenario-grid" id="scenarios">
                <!-- Scenarios will be populated by JavaScript -->
            </div>
            
            <div style="text-align: center; margin-top: 40px; color: #6c757d; font-size: 0.9em;">
                <p>ü§ñ Auto-generated MLPerf benchmark suite report</p>
            </div>
        </div>
    </div>
    
    <script>
        // Populate scenarios dynamically
        const scenarios = [
            {name: 'SingleStream', samples: 100, description: 'Single query processing test'},
            {name: 'Offline', samples: 11490, description: 'Full dataset batch processing'}, 
            {name: 'Server', samples: 1000, description: 'Server scenario simulation'}
        ];
        
        const scenarioGrid = document.getElementById('scenarios');
        scenarios.forEach(scenario => {
            const card = document.createElement('div');
            card.className = 'scenario-card';
            card.innerHTML = `
                <div class="scenario-title">üìä ${scenario.name}</div>
                <div class="metric">Samples: <span class="metric-value">${scenario.samples}</span></div>
                <div class="metric">Description: <span class="metric-value">${scenario.description}</span></div>
                <div class="metric">Status: <span class="metric-value">See individual reports</span></div>
            `;
            scenarioGrid.appendChild(card);
        });
    </script>
</body>
</html>
EOF
    
    # Replace placeholders
    sed -i "s/TIMESTAMP_PLACEHOLDER/$(date '+%B %d, %Y at %H:%M:%S')/g" "$summary_file"
    sed -i "s/DURATION_PLACEHOLDER/$total_duration/g" "$summary_file"
    sed -i "s/FAILED_PLACEHOLDER/$failed_scenarios/g" "$summary_file"
    
    echo -e "${GREEN}üìã Summary report: $(basename "$summary_file")${NC}"
}

show_results() {
    echo -e "${BLUE}üìä Benchmark Results Summary${NC}"
    echo "================================"
    
    if [ -d "$RESULTS_DIR" ]; then
        echo -e "${GREEN}üìÅ Results directory: $RESULTS_DIR${NC}"
        
        # Count result files
        local result_files=$(find "$RESULTS_DIR" -name "*.json" -o -name "*.html" | wc -l)
        echo -e "${GREEN}üìã Generated files: $result_files${NC}"
        
        # Show scenario results
        echo -e "${YELLOW}üéØ Scenario Results:${NC}"
        for scenario in SingleStream Offline Server; do
            if [ -d "$RESULTS_DIR/$scenario" ]; then
                echo -e "${GREEN}   ‚úÖ $scenario: Results available${NC}"
                local html_count=$(find "$RESULTS_DIR/$scenario" -name "*.html" | wc -l)
                local json_count=$(find "$RESULTS_DIR/$scenario" -name "*.json" | wc -l)
                echo "      üìÑ JSON reports: $json_count | üåê HTML reports: $html_count"
            else
                echo -e "${RED}   ‚ùå $scenario: No results found${NC}"
            fi
        done
        
        # Show summary report
        local summary_reports=$(find "$RESULTS_DIR" -name "benchmark_summary_*.html" | head -1)
        if [ -n "$summary_reports" ]; then
            echo -e "${YELLOW}üìã Summary Report: $(basename "$summary_reports")${NC}"
        fi
        
        echo ""
        echo -e "${BLUE}üîç To view results:${NC}"
        echo "   ‚Ä¢ Open HTML reports in a web browser"
        echo "   ‚Ä¢ Check JSON files for detailed metrics"
        echo "   ‚Ä¢ Individual scenario results in subdirectories"
        
    else
        echo -e "${RED}‚ùå No results directory found${NC}"
    fi
}

cleanup() {
    echo -e "${YELLOW}üßπ Cleaning up temporary files...${NC}"
    # Clean up any temporary files if needed
}

# Set up cleanup trap
trap cleanup EXIT

# Main execution
main() {
    print_banner
    check_prerequisites
    run_all_scenarios
    show_results
    
    echo -e "${GREEN}üéâ MLPerf optimized benchmark suite completed successfully!${NC}"
    echo -e "${BLUE}üìÅ All results saved to: $RESULTS_DIR${NC}"
}

# Show help
show_help() {
    echo "MLPerf Optimized All-Scenarios Benchmark Runner"
    echo ""
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "This script runs optimized MLPerf benchmarks with:"
    echo "  ‚Ä¢ 8.7x performance improvement over baseline"
    echo "  ‚Ä¢ Automatic HTML report generation"
    echo "  ‚Ä¢ Multiple scenario testing (SingleStream, Offline, Server)"
    echo "  ‚Ä¢ Comprehensive results summary"
    echo ""
    echo "Examples:"
    echo "  $0                    # Run all scenarios"
    echo "  $0 --help           # Show this help"
    echo ""
    echo "Requirements:"
    echo "  ‚Ä¢ Python 3 with torch, vllm, datasets, rouge-score"
    echo "  ‚Ä¢ NVIDIA GPU with CUDA support"
    echo "  ‚Ä¢ Optimized benchmark scripts in current directory"
}

# Parse command line arguments
case "${1:-run}" in
    "help"|"--help"|"-h")
        show_help
        ;;
    "run"|"")
        main
        ;;
    *)
        echo -e "${RED}‚ùå Unknown command: $1${NC}"
        show_help
        exit 1
        ;;
esac