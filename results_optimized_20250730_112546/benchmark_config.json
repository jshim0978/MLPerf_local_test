{
    "timestamp": "2025-07-30T11:25:50+09:00",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "optimization": "VLLM + CUDA Graphs + Batching",
    "expected_speedup": "8.7x",
    "baseline_throughput": 0.75,
    "scenarios": [
        {"name": "SingleStream", "samples": 100, "description": "Single query processing"},
        {"name": "Offline", "samples": 1000, "description": "Batch processing optimization"},
        {"name": "Server", "samples": 500, "description": "Concurrent request handling"}
    ]
}
