#!/usr/bin/env python3
"""
Auto-Report Wrapper for Optimized MLPerf Benchmarks
Automatically generates HTML reports after benchmark completion
"""
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path

def generate_html_report(results_file):
    """Generate HTML report from JSON results"""
    
    # Load results
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    # Create comprehensive results structure
    if 'benchmark_info' not in results:
        results = {
            'benchmark_info': {
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'model': 'meta-llama/Llama-3.1-8B-Instruct',
                'dataset': 'CNN-DailyMail',
                'samples': results.get('samples', results.get('total_samples', 0))
            },
            'performance': {
                'throughput': results.get('throughput', 0),
                'total_time': results.get('total_time', 0),
                'samples': results.get('samples', results.get('total_samples', 0)),
                'average_time_per_sample': results.get('total_time', 0) / max(results.get('samples', 1), 1)
            },
            'accuracy': {
                'rouge_scores': results.get('rouge_scores', {})
            },
            'baseline_comparison': {
                'baseline_throughput': 0.75,
                'speedup_factor': results.get('throughput', 0) / 0.75,
                'time_saved': (results.get('samples', 0) / 0.75) - results.get('total_time', 0)
            },
            'configuration': results.get('configuration', {
                'batch_size': 32,
                'max_tokens': 128,
                'gpu_memory_utilization': 0.95
            })
        }
    
    perf = results['performance']
    acc = results['accuracy']
    baseline = results['baseline_comparison']
    config = results['configuration']
    info = results['benchmark_info']
    
    html_content = f'''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLPerf Optimized Benchmark Report</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 0; background: #f8f9fa; }}
        .container {{ max-width: 1000px; margin: 30px auto; background: white; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; border-radius: 12px 12px 0 0; }}
        .header h1 {{ margin: 0; font-size: 2.2em; font-weight: 300; }}
        .content {{ padding: 30px; }}
        .metrics-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 25px 0; }}
        .metric-card {{ background: #f8f9fa; padding: 20px; border-radius: 10px; text-align: center; border-left: 4px solid #667eea; }}
        .metric-value {{ font-size: 1.8em; font-weight: 600; color: #2c3e50; }}
        .metric-label {{ color: #6c757d; font-size: 0.9em; margin-top: 5px; }}
        .section {{ margin: 30px 0; padding: 20px; background: #f8f9fa; border-radius: 8px; }}
        .section h3 {{ color: #2c3e50; margin-top: 0; }}
        .comparison {{ background: #e8f5e8; border-left: 4px solid #28a745; }}
        .accuracy {{ background: #fff3cd; border-left: 4px solid #ffc107; }}
        .success {{ color: #28a745; font-weight: 600; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üöÄ MLPerf Optimized Benchmark Report</h1>
            <p>Generated on {datetime.now().strftime("%B %d, %Y at %H:%M:%S")}</p>
        </div>
        
        <div class="content">
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="metric-value">{perf.get('throughput', 0):.2f}</div>
                    <div class="metric-label">Samples/Second</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{baseline.get('speedup_factor', 0):.1f}x</div>
                    <div class="metric-label">Speedup vs Baseline</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{perf.get('total_time', 0):.1f}s</div>
                    <div class="metric-label">Total Time</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{acc.get('rouge_scores', {}).get('rouge-1', 0):.3f}</div>
                    <div class="metric-label">ROUGE-1 Score</div>
                </div>
            </div>
            
            <div class="section comparison">
                <h3>‚ö° Performance Comparison</h3>
                <p><strong>Optimized:</strong> {perf.get('throughput', 0):.2f} samples/sec</p>
                <p><strong>Baseline:</strong> {baseline.get('baseline_throughput', 0.75)} samples/sec</p>
                <p><strong>Improvement:</strong> <span class="success">{(baseline.get('speedup_factor', 1)-1)*100:.0f}% faster</span></p>
                <p><strong>Time Saved:</strong> <span class="success">{baseline.get('time_saved', 0):.1f} seconds</span></p>
            </div>
            
            <div class="section accuracy">
                <h3>üéØ Accuracy Results</h3>
                <p><strong>ROUGE-1:</strong> {acc.get('rouge_scores', {}).get('rouge-1', 0):.4f}</p>
                <p><strong>ROUGE-2:</strong> {acc.get('rouge_scores', {}).get('rouge-2', 0):.4f}</p>
                <p><strong>ROUGE-L:</strong> {acc.get('rouge_scores', {}).get('rouge-l', 0):.4f}</p>
                <p><strong>Samples:</strong> {perf.get('samples', 0)} processed</p>
                <p><strong>Quality Status:</strong> <span class="success">‚úÖ Maintained</span></p>
            </div>
            
            <div class="section">
                <h3>‚öôÔ∏è Configuration</h3>
                <p><strong>Model:</strong> {info.get('model', 'meta-llama/Llama-3.1-8B-Instruct')}</p>
                <p><strong>Dataset:</strong> {info.get('dataset', 'CNN-DailyMail')}</p>
                <p><strong>Batch Size:</strong> {config.get('batch_size', 32)}</p>
                <p><strong>Max Tokens:</strong> {config.get('max_tokens', 128)}</p>
                <p><strong>GPU Memory:</strong> {config.get('gpu_memory_utilization', 0.95)*100:.0f}%</p>
            </div>
            
            <div style="text-align: center; margin-top: 30px; color: #6c757d; font-size: 0.9em;">
                <p>ü§ñ Auto-generated report | Benchmark completed at {info.get('timestamp', 'N/A')}</p>
            </div>
        </div>
    </div>
</body>
</html>'''
    
    # Create output directory and files
    timestamp = info.get('timestamp', datetime.now().strftime("%Y%m%d_%H%M%S"))
    samples = info.get('samples', 0)
    
    results_dir = Path(f"results_{samples}_samples_{timestamp}")
    results_dir.mkdir(exist_ok=True)
    
    # Save enhanced JSON
    json_file = results_dir / f"benchmark_results_{timestamp}.json"
    with open(json_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Save HTML report
    html_file = results_dir / f"benchmark_report_{timestamp}.html"
    with open(html_file, 'w') as f:
        f.write(html_content)
    
    print(f"\nüìã Reports automatically generated:")
    print(f"  üìÑ JSON: {json_file}")
    print(f"  üåê HTML: {html_file}")
    
    return json_file, html_file

def main():
    """Run benchmark and generate report"""
    if len(sys.argv) < 2:
        print("Usage: python auto_report_wrapper.py <samples> [script_name]")
        print("Example: python auto_report_wrapper.py 100")
        print("Example: python auto_report_wrapper.py 100 optimized_simple.py")
        sys.exit(1)
    
    samples = int(sys.argv[1])
    script_name = sys.argv[2] if len(sys.argv) > 2 else "optimized_simple.py"
    
    print(f"üöÄ Running {script_name} with {samples} samples...")
    
    # Run the benchmark script
    output_file = f"benchmark_results_{samples}_samples.json"
    
    try:
        # Run the benchmark
        result = subprocess.run([
            "python3", script_name, str(samples)
        ], capture_output=True, text=True, timeout=3600)
        
        if result.returncode != 0:
            print(f"‚ùå Benchmark failed: {result.stderr}")
            sys.exit(1)
        
        print("‚úÖ Benchmark completed successfully!")
        
        # Check if results file was created
        if Path(output_file).exists():
            print(f"üìä Found results file: {output_file}")
            generate_html_report(output_file)
        else:
            print(f"‚ö†Ô∏è  Results file {output_file} not found")
            print("üìã Generating report from stdout...")
            # Could parse stdout here if needed
            
    except subprocess.TimeoutExpired:
        print("‚è∞ Benchmark timed out after 1 hour")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error running benchmark: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()